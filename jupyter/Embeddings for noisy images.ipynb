{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "30f32ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from skimage.transform import resize\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41231c1b",
   "metadata": {},
   "source": [
    "# Data requirements\n",
    "* we need: image name or id (to distinguish link back to the main df),simple_tsne, meaningful_tsne, pca,bottleneck_activations_embedding, IoU values, image path, prediction path\n",
    "* existing information: name, dataset, label_path, class ratios,\n",
    "* not useful for continuous domains: similar_image_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0db502",
   "metadata": {},
   "source": [
    "# Note: should be able to modify noise levels and sample number, and just re-run everything"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb7ed15",
   "metadata": {},
   "source": [
    "1. Load images and the labels (**Cityscapes for now**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3ed9cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the relavant data\n",
    "pickle_file = os.path.join(\"D:\\zsh\\graduation\\Graduation-project-domain-shift-image-2-image\\dataset\\\\5_classes_preprocessed\", \"validation_images.pkl\")\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    X_test = pickle.load(f)\n",
    "\n",
    "pickle_file = os.path.join(\"D:\\zsh\\graduation\\Graduation-project-domain-shift-image-2-image\\dataset\\\\5_classes_preprocessed\", \"validation_label_classes.pkl\")\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    Y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4e08735",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path_cityscapes = \"D:\\zsh\\graduation\\Graduation-project-domain-shift-image-2-image\\dataset\\original_cityscapes_inputs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec456d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(X_test)\n",
    "Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a15eef8",
   "metadata": {},
   "source": [
    "verify that the images are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "520816ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_imaage_path = os.path.join(image_path_cityscapes,\"0.jpeg\")\n",
    "# example = Image.open(example_imaage_path).convert(\"RGB\")\n",
    "# example = np.array(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8bbd9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(example[0])\n",
    "# print(X_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f40ebf9",
   "metadata": {},
   "source": [
    "use the first 100 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "feadf9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_number=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "280797b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_images = X_test[:sample_number]\n",
    "subset_labels = Y_test[:sample_number]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f446e236",
   "metadata": {},
   "source": [
    "Attempt at sampling, and then realizing this creates more difficulties later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80ece635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_number = 100\n",
    "\n",
    "# cityscapes_names= os.listdir(image_path_cityscapes)\n",
    "# random.seed(55)\n",
    "# cityscapes_names_sample = random.sample(cityscapes_names,sample_number)\n",
    "\n",
    "# sample_indices = []\n",
    "# for name in cityscapes_names_sample:\n",
    "#     index = int(name.split(\".\")[0])\n",
    "#     sample_indices.append(index)\n",
    "\n",
    "# sample_indices.sort()\n",
    "# print(sample_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a24ebbb",
   "metadata": {},
   "source": [
    "1. Need to create and save images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c0d085d",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_levels = [0,0.1,0.2,0.3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f629ea",
   "metadata": {},
   "source": [
    "A function to transform original data to noisy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79b6d329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dataset(original_dataset,noise_level=0):\n",
    "    new_dataset = []\n",
    "    if noise_level == 0:\n",
    "        return original_dataset\n",
    "    for image in original_dataset:\n",
    "        new_image = image+(noise_level*np.random.normal(0, (image.max() - image.min())/6., image.shape)) # (mean, sigma, image_shape)\n",
    "        # sometimes the random results are out of the pixel range, portraying very weird values\n",
    "        new_image = np.clip(new_image,0,255).astype(\"uint8\")\n",
    "        new_dataset.append(new_image)\n",
    "    new_dataset = np.array(new_dataset)\n",
    "    return new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "708e82e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image_list = [transform_dataset(subset_images,noise_levels[i]) for i in range(len(noise_levels))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2012fd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image_array = np.array(input_image_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a357c0",
   "metadata": {},
   "source": [
    "* Save images in different folders (each labeled with the noise level)\n",
    "* Save the corresponding paths to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3fb2721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df with real vs synthetic data\n",
    "# use the label path of original df later\n",
    "original_df = pd.read_csv(\"D:\\zsh\\graduation\\Graduation-project-domain-shift-image-2-image\\system_df_v2.csv\")\n",
    "ratio_columns = original_df.filter(like=\"_ratio\").columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dabccafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = \"D:\\zsh\\graduation\\Graduation-project-domain-shift-image-2-image\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "451094c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_image_folder = \"D:\\zsh\\graduation\\Graduation-project-domain-shift-image-2-image\\dataset\\\\noise_data\\images\"\n",
    "path_list = []\n",
    "for i,noisy_images in enumerate(input_image_array):\n",
    "    noise_level = noise_levels[i]\n",
    "    for j,image in enumerate(noisy_images):\n",
    "        img_object = Image.fromarray(image.astype(np.uint8))\n",
    "        name = str(j)+\".jpeg\"\n",
    "        img_path = os.path.join(noisy_image_folder,str(noise_level),name)\n",
    "        img_path_rel = os.path.relpath(img_path, start)\n",
    "        img_object.save(img_path)\n",
    "        original_row = original_df.loc[original_df[\"name\"]==name]\n",
    "        label_path = original_row[\"label_path\"].values[0]\n",
    "        instance=[name,img_path_rel,label_path]\n",
    "        for column in ratio_columns:\n",
    "            instance.append(original_row[column].values[0])        \n",
    "        \n",
    "        path_list.append(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aaaf61b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names_initial = [\"name\", \"image_path\",\"label_path\"]\n",
    "column_names_initial.extend(ratio_columns)\n",
    "df = pd.DataFrame(path_list,columns = column_names_initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2bb34602",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"dataset\"]=\"Cityscapes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e77f3b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>image_path</th>\n",
       "      <th>label_path</th>\n",
       "      <th>other_ratio</th>\n",
       "      <th>road_ratio</th>\n",
       "      <th>sidewalk_ratio</th>\n",
       "      <th>vegetation_ratio</th>\n",
       "      <th>sky_ratio</th>\n",
       "      <th>car_ratio</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.jpeg</td>\n",
       "      <td>dataset\\noise_data\\images\\0\\0.jpeg</td>\n",
       "      <td>dataset\\cityscapes_labels_sample\\0.jpeg</td>\n",
       "      <td>0.372009</td>\n",
       "      <td>0.320770</td>\n",
       "      <td>0.100510</td>\n",
       "      <td>0.183990</td>\n",
       "      <td>0.020248</td>\n",
       "      <td>0.002472</td>\n",
       "      <td>Cityscapes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.jpeg</td>\n",
       "      <td>dataset\\noise_data\\images\\0\\1.jpeg</td>\n",
       "      <td>dataset\\cityscapes_labels_sample\\1.jpeg</td>\n",
       "      <td>0.539703</td>\n",
       "      <td>0.414398</td>\n",
       "      <td>0.041168</td>\n",
       "      <td>0.000565</td>\n",
       "      <td>0.004166</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Cityscapes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.jpeg</td>\n",
       "      <td>dataset\\noise_data\\images\\0\\2.jpeg</td>\n",
       "      <td>dataset\\cityscapes_labels_sample\\2.jpeg</td>\n",
       "      <td>0.462753</td>\n",
       "      <td>0.354187</td>\n",
       "      <td>0.029633</td>\n",
       "      <td>0.121323</td>\n",
       "      <td>0.018356</td>\n",
       "      <td>0.013748</td>\n",
       "      <td>Cityscapes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.jpeg</td>\n",
       "      <td>dataset\\noise_data\\images\\0\\3.jpeg</td>\n",
       "      <td>dataset\\cityscapes_labels_sample\\3.jpeg</td>\n",
       "      <td>0.425674</td>\n",
       "      <td>0.381119</td>\n",
       "      <td>0.065292</td>\n",
       "      <td>0.101120</td>\n",
       "      <td>0.016296</td>\n",
       "      <td>0.010498</td>\n",
       "      <td>Cityscapes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.jpeg</td>\n",
       "      <td>dataset\\noise_data\\images\\0\\4.jpeg</td>\n",
       "      <td>dataset\\cityscapes_labels_sample\\4.jpeg</td>\n",
       "      <td>0.400284</td>\n",
       "      <td>0.329453</td>\n",
       "      <td>0.090469</td>\n",
       "      <td>0.174393</td>\n",
       "      <td>0.001465</td>\n",
       "      <td>0.003937</td>\n",
       "      <td>Cityscapes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name                          image_path  \\\n",
       "0  0.jpeg  dataset\\noise_data\\images\\0\\0.jpeg   \n",
       "1  1.jpeg  dataset\\noise_data\\images\\0\\1.jpeg   \n",
       "2  2.jpeg  dataset\\noise_data\\images\\0\\2.jpeg   \n",
       "3  3.jpeg  dataset\\noise_data\\images\\0\\3.jpeg   \n",
       "4  4.jpeg  dataset\\noise_data\\images\\0\\4.jpeg   \n",
       "\n",
       "                                label_path  other_ratio  road_ratio  \\\n",
       "0  dataset\\cityscapes_labels_sample\\0.jpeg     0.372009    0.320770   \n",
       "1  dataset\\cityscapes_labels_sample\\1.jpeg     0.539703    0.414398   \n",
       "2  dataset\\cityscapes_labels_sample\\2.jpeg     0.462753    0.354187   \n",
       "3  dataset\\cityscapes_labels_sample\\3.jpeg     0.425674    0.381119   \n",
       "4  dataset\\cityscapes_labels_sample\\4.jpeg     0.400284    0.329453   \n",
       "\n",
       "   sidewalk_ratio  vegetation_ratio  sky_ratio  car_ratio     dataset  \n",
       "0        0.100510          0.183990   0.020248   0.002472  Cityscapes  \n",
       "1        0.041168          0.000565   0.004166   0.000000  Cityscapes  \n",
       "2        0.029633          0.121323   0.018356   0.013748  Cityscapes  \n",
       "3        0.065292          0.101120   0.016296   0.010498  Cityscapes  \n",
       "4        0.090469          0.174393   0.001465   0.003937  Cityscapes  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918a8071",
   "metadata": {},
   "source": [
    "# Embedding of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d66fd38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.847570815996223\n"
     ]
    }
   ],
   "source": [
    "simple_combined_embedding =np.reshape(input_image_array,(len(input_image_array)*sample_number, 256*256*3))\n",
    "\n",
    "pca_50 = PCA(n_components=50)\n",
    "pca_embedding = pca_50.fit_transform(simple_combined_embedding)\n",
    "print(np.sum(pca_50.explained_variance_ratio_))\n",
    "tsne = TSNE()\n",
    "simple_tsne_embedding = tsne.fit_transform(pca_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ac1afdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22266320806989054\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca_embedding = pca.fit_transform(simple_combined_embedding)\n",
    "print(np.sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7b4f2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\zsh\\graduation\\ViTs-vs-CNNs\n"
     ]
    }
   ],
   "source": [
    "cd D:\\zsh\\graduation\\ViTs-vs-CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca54a37a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import global_val\n",
    "\n",
    "from models.ghost_bn import GhostBN2D_ADV\n",
    "from models.advresnet_gbn_gelu import Affine\n",
    "import models.advresnet_gbn_gelu as advres\n",
    "from main_adv_res import EightBN\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "classifier_model = advres.__dict__[\"resnet50\"](norm_layer = EightBN)\n",
    "weights_path = \"D:\\zsh\\graduation\\Graduation-project-domain-shift-image-2-image\\models\\\\advres50_gelu.pth\"\n",
    "weight_dict = torch.load(weights_path,map_location=device)[\"model\"]\n",
    "\n",
    "classifier_model.load_state_dict(weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "481649fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\zsh\\graduation\\Graduation-project-domain-shift-image-2-image\\jupyter\n"
     ]
    }
   ],
   "source": [
    "cd D:\\zsh\\graduation\\Graduation-project-domain-shift-image-2-image\\jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fb1c62",
   "metadata": {},
   "source": [
    "(added second_part_embedding back after the last run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2dba5c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_representation_for_image(image,label):\n",
    "    # resize the image and transform it to tensor # [256,256,3] -> [224,224,3] \n",
    "    resized_image = resize(image, (224, 224,3))\n",
    "    resized_image_tensor = torch.from_numpy(resized_image)\n",
    "    resized_image_tensor = torch.permute(resized_image_tensor, (2, 0, 1))\n",
    "    resized_image_tensor = resized_image_tensor[None,:] # [3,224,224] -> [1,3,224,224] (because model takes 4D input)\n",
    "    \n",
    "    # dictionary for the activations\n",
    "    activations = {}\n",
    "\n",
    "    def get_activations(name):\n",
    "        def hook(model, input, output):\n",
    "            activations[name] = output.detach()\n",
    "        return hook\n",
    "\n",
    "    # use the output of (avgpool) layer because it is the same as the input of last layer\n",
    "    h = classifier_model.avgpool.register_forward_hook(get_activations(\"input_last_layer\"))\n",
    "\n",
    "    classifier_model.eval()\n",
    "    classifier_model.sing=True\n",
    "    classifier_model.training=False\n",
    "    out = classifier_model(resized_image_tensor.float(),label)\n",
    "    \n",
    "    # remove the hook\n",
    "    h.remove()\n",
    "\n",
    "    first_part_embedding = torch.flatten(activations[\"input_last_layer\"][0])\n",
    "    # the second part of the embedding is size of each class\n",
    "    #  (this section of code is overlapping with generating class distribution, improve this for a better result)   \n",
    "    second_part_embedding=[]\n",
    "    for i in range(6):\n",
    "        class_size = np.count_nonzero(label == i)\n",
    "        second_part_embedding.append(class_size)\n",
    "    embedding = np.concatenate((first_part_embedding.numpy(),np.array(second_part_embedding)),axis = 0)\n",
    "    \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a7a81b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_images = input_image_array.reshape(((len(input_image_array)*sample_number, 256,256,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9412744d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 256, 256, 3)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e51aa950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9206016738680827\n"
     ]
    }
   ],
   "source": [
    "embedding_list = []\n",
    "for i, image in enumerate(combined_images): # i is not used, could remove the enumerate\n",
    "    embedding = get_representation_for_image(image,\"__\") # y_label is never used, so just use replacement\n",
    "    embedding_list.append(embedding)\n",
    "\n",
    "embedding_arr = np.array(embedding_list)\n",
    "pca_50 = PCA(n_components=50)\n",
    "pca_embedding = pca_50.fit_transform(embedding_arr)\n",
    "print(np.sum(pca_50.explained_variance_ratio_))\n",
    "# pca = PCA(n_components=2)\n",
    "tsne = TSNE()\n",
    "meaningful_tsne_embedding = tsne.fit_transform(pca_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ef2e2e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 2)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meaningful_tsne_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "466eb3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_2= PCA(n_components=2)\n",
    "pca_embedding = pca_2.fit_transform(embedding_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "616218d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_embedding = np.concatenate((simple_tsne_embedding,meaningful_tsne_embedding,pca_embedding),axis=1)\n",
    "df_input_embedding = pd.DataFrame(full_embedding, columns=[\"simple_tsne_1\",\"simple_tsne_2\",\"meaningful_tsne_1\",\"meaningful_tsne_2\",\"pca_1\",\"pca_2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ac7944e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat((df,df_input_embedding),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "49bd7d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>image_path</th>\n",
       "      <th>label_path</th>\n",
       "      <th>other_ratio</th>\n",
       "      <th>road_ratio</th>\n",
       "      <th>sidewalk_ratio</th>\n",
       "      <th>vegetation_ratio</th>\n",
       "      <th>sky_ratio</th>\n",
       "      <th>car_ratio</th>\n",
       "      <th>dataset</th>\n",
       "      <th>simple_tsne_1</th>\n",
       "      <th>simple_tsne_2</th>\n",
       "      <th>meaningful_tsne_1</th>\n",
       "      <th>meaningful_tsne_2</th>\n",
       "      <th>pca_1</th>\n",
       "      <th>pca_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.jpeg</td>\n",
       "      <td>dataset\\noise_data\\images\\0\\0.jpeg</td>\n",
       "      <td>dataset\\cityscapes_labels_sample\\0.jpeg</td>\n",
       "      <td>0.372009</td>\n",
       "      <td>0.320770</td>\n",
       "      <td>0.100510</td>\n",
       "      <td>0.183990</td>\n",
       "      <td>0.020248</td>\n",
       "      <td>0.002472</td>\n",
       "      <td>Cityscapes</td>\n",
       "      <td>9.297244</td>\n",
       "      <td>14.170387</td>\n",
       "      <td>-14.293534</td>\n",
       "      <td>-5.071721</td>\n",
       "      <td>2.543226</td>\n",
       "      <td>5.683888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.jpeg</td>\n",
       "      <td>dataset\\noise_data\\images\\0\\1.jpeg</td>\n",
       "      <td>dataset\\cityscapes_labels_sample\\1.jpeg</td>\n",
       "      <td>0.539703</td>\n",
       "      <td>0.414398</td>\n",
       "      <td>0.041168</td>\n",
       "      <td>0.000565</td>\n",
       "      <td>0.004166</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Cityscapes</td>\n",
       "      <td>-7.711626</td>\n",
       "      <td>-12.276681</td>\n",
       "      <td>-9.466860</td>\n",
       "      <td>15.319448</td>\n",
       "      <td>-0.228557</td>\n",
       "      <td>-4.261134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.jpeg</td>\n",
       "      <td>dataset\\noise_data\\images\\0\\2.jpeg</td>\n",
       "      <td>dataset\\cityscapes_labels_sample\\2.jpeg</td>\n",
       "      <td>0.462753</td>\n",
       "      <td>0.354187</td>\n",
       "      <td>0.029633</td>\n",
       "      <td>0.121323</td>\n",
       "      <td>0.018356</td>\n",
       "      <td>0.013748</td>\n",
       "      <td>Cityscapes</td>\n",
       "      <td>-2.667099</td>\n",
       "      <td>3.300517</td>\n",
       "      <td>2.911404</td>\n",
       "      <td>-3.285340</td>\n",
       "      <td>0.035510</td>\n",
       "      <td>1.616864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.jpeg</td>\n",
       "      <td>dataset\\noise_data\\images\\0\\3.jpeg</td>\n",
       "      <td>dataset\\cityscapes_labels_sample\\3.jpeg</td>\n",
       "      <td>0.425674</td>\n",
       "      <td>0.381119</td>\n",
       "      <td>0.065292</td>\n",
       "      <td>0.101120</td>\n",
       "      <td>0.016296</td>\n",
       "      <td>0.010498</td>\n",
       "      <td>Cityscapes</td>\n",
       "      <td>16.245871</td>\n",
       "      <td>-3.283362</td>\n",
       "      <td>-28.758741</td>\n",
       "      <td>-0.009148</td>\n",
       "      <td>3.053359</td>\n",
       "      <td>-0.375224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.jpeg</td>\n",
       "      <td>dataset\\noise_data\\images\\0\\4.jpeg</td>\n",
       "      <td>dataset\\cityscapes_labels_sample\\4.jpeg</td>\n",
       "      <td>0.400284</td>\n",
       "      <td>0.329453</td>\n",
       "      <td>0.090469</td>\n",
       "      <td>0.174393</td>\n",
       "      <td>0.001465</td>\n",
       "      <td>0.003937</td>\n",
       "      <td>Cityscapes</td>\n",
       "      <td>7.521536</td>\n",
       "      <td>10.013210</td>\n",
       "      <td>16.275972</td>\n",
       "      <td>4.410336</td>\n",
       "      <td>-6.612169</td>\n",
       "      <td>5.832494</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name                          image_path  \\\n",
       "0  0.jpeg  dataset\\noise_data\\images\\0\\0.jpeg   \n",
       "1  1.jpeg  dataset\\noise_data\\images\\0\\1.jpeg   \n",
       "2  2.jpeg  dataset\\noise_data\\images\\0\\2.jpeg   \n",
       "3  3.jpeg  dataset\\noise_data\\images\\0\\3.jpeg   \n",
       "4  4.jpeg  dataset\\noise_data\\images\\0\\4.jpeg   \n",
       "\n",
       "                                label_path  other_ratio  road_ratio  \\\n",
       "0  dataset\\cityscapes_labels_sample\\0.jpeg     0.372009    0.320770   \n",
       "1  dataset\\cityscapes_labels_sample\\1.jpeg     0.539703    0.414398   \n",
       "2  dataset\\cityscapes_labels_sample\\2.jpeg     0.462753    0.354187   \n",
       "3  dataset\\cityscapes_labels_sample\\3.jpeg     0.425674    0.381119   \n",
       "4  dataset\\cityscapes_labels_sample\\4.jpeg     0.400284    0.329453   \n",
       "\n",
       "   sidewalk_ratio  vegetation_ratio  sky_ratio  car_ratio     dataset  \\\n",
       "0        0.100510          0.183990   0.020248   0.002472  Cityscapes   \n",
       "1        0.041168          0.000565   0.004166   0.000000  Cityscapes   \n",
       "2        0.029633          0.121323   0.018356   0.013748  Cityscapes   \n",
       "3        0.065292          0.101120   0.016296   0.010498  Cityscapes   \n",
       "4        0.090469          0.174393   0.001465   0.003937  Cityscapes   \n",
       "\n",
       "   simple_tsne_1  simple_tsne_2  meaningful_tsne_1  meaningful_tsne_2  \\\n",
       "0       9.297244      14.170387         -14.293534          -5.071721   \n",
       "1      -7.711626     -12.276681          -9.466860          15.319448   \n",
       "2      -2.667099       3.300517           2.911404          -3.285340   \n",
       "3      16.245871      -3.283362         -28.758741          -0.009148   \n",
       "4       7.521536      10.013210          16.275972           4.410336   \n",
       "\n",
       "      pca_1     pca_2  \n",
       "0  2.543226  5.683888  \n",
       "1 -0.228557 -4.261134  \n",
       "2  0.035510  1.616864  \n",
       "3  3.053359 -0.375224  \n",
       "4 -6.612169  5.832494  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2920bfd2",
   "metadata": {},
   "source": [
    "# Define the Dataloader and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "73a953bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityscapesDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, images, labels, noise_level = 0):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.noise_level = noise_level\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        label = self.labels[index]\n",
    "        if self.noise_level!=0:\n",
    "            image = image+(self.noise_level*np.random.normal(0, (image.max() - image.min()), image.shape)).astype(\"uint8\") # (mean, sigma, image_shape)\n",
    "        image = self.transform(image)\n",
    "        label = torch.from_numpy(label).long()\n",
    "        return image, label\n",
    "        \n",
    "    def transform(self, image):\n",
    "        transform_ops = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)) # normalize to control the \"dynamic range\" of activations of different layers\n",
    "        ])\n",
    "        return transform_ops(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "449e4f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes):\n",
    "        super(UNet, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.contracting_11 = self.conv_block(in_channels=3, out_channels=64)\n",
    "        self.contracting_12 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.contracting_21 = self.conv_block(in_channels=64, out_channels=128)\n",
    "        self.contracting_22 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.contracting_31 = self.conv_block(in_channels=128, out_channels=256)\n",
    "        self.contracting_32 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.contracting_41 = self.conv_block(in_channels=256, out_channels=512)\n",
    "        self.contracting_42 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.middle = self.conv_block(in_channels=512, out_channels=1024)\n",
    "        self.expansive_11 = nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.expansive_12 = self.conv_block(in_channels=1024, out_channels=512)\n",
    "        self.expansive_21 = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.expansive_22 = self.conv_block(in_channels=512, out_channels=256)\n",
    "        self.expansive_31 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.expansive_32 = self.conv_block(in_channels=256, out_channels=128)\n",
    "        self.expansive_41 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.expansive_42 = self.conv_block(in_channels=128, out_channels=64)\n",
    "        self.output = nn.Conv2d(in_channels=64, out_channels=num_classes, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        block = nn.Sequential(nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.BatchNorm2d(num_features=out_channels),\n",
    "                                    nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.BatchNorm2d(num_features=out_channels))\n",
    "        return block\n",
    "    \n",
    "    def forward(self, X):\n",
    "        contracting_11_out = self.contracting_11(X) # [-1, 64, 256, 256]\n",
    "        contracting_12_out = self.contracting_12(contracting_11_out) # [-1, 64, 128, 128]\n",
    "        contracting_21_out = self.contracting_21(contracting_12_out) # [-1, 128, 128, 128]\n",
    "        contracting_22_out = self.contracting_22(contracting_21_out) # [-1, 128, 64, 64]\n",
    "        contracting_31_out = self.contracting_31(contracting_22_out) # [-1, 256, 64, 64]\n",
    "        contracting_32_out = self.contracting_32(contracting_31_out) # [-1, 256, 32, 32]\n",
    "        contracting_41_out = self.contracting_41(contracting_32_out) # [-1, 512, 32, 32]\n",
    "        contracting_42_out = self.contracting_42(contracting_41_out) # [-1, 512, 16, 16]\n",
    "        middle_out = self.middle(contracting_42_out) # [-1, 1024, 16, 16]\n",
    "        expansive_11_out = self.expansive_11(middle_out) # [-1, 512, 32, 32]\n",
    "        expansive_12_out = self.expansive_12(torch.cat((expansive_11_out, contracting_41_out), dim=1)) # [-1, 1024, 32, 32] -> [-1, 512, 32, 32]\n",
    "        expansive_21_out = self.expansive_21(expansive_12_out) # [-1, 256, 64, 64]\n",
    "        expansive_22_out = self.expansive_22(torch.cat((expansive_21_out, contracting_31_out), dim=1)) # [-1, 512, 64, 64] -> [-1, 256, 64, 64]\n",
    "        expansive_31_out = self.expansive_31(expansive_22_out) # [-1, 128, 128, 128]\n",
    "        expansive_32_out = self.expansive_32(torch.cat((expansive_31_out, contracting_21_out), dim=1)) # [-1, 256, 128, 128] -> [-1, 128, 128, 128]\n",
    "        expansive_41_out = self.expansive_41(expansive_32_out) # [-1, 64, 256, 256]\n",
    "        expansive_42_out = self.expansive_42(torch.cat((expansive_41_out, contracting_11_out), dim=1)) # [-1, 128, 256, 256] -> [-1, 64, 256, 256]\n",
    "        output_out = self.output(expansive_42_out) # [-1, num_classes, 256, 256]\n",
    "        return output_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4f15f007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"D:\\zsh\\graduation\\Graduation-project-domain-shift-image-2-image\\models\\\\5-classes-U-Net-2023-03-09.pth\"\n",
    "\n",
    "device = torch.device('cpu')\n",
    "model_ = UNet(num_classes=6).to(device)\n",
    "model_.load_state_dict(torch.load(model_path,map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b5b30940",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IoU_per_class(prediction,label):\n",
    "    prediction = prediction.detach().numpy()\n",
    "    label = label.detach().numpy()\n",
    "    # Loop over each class\n",
    "    iou_list = []\n",
    "    # Flatten label and class arrays\n",
    "    flat_prediction = prediction.flatten()\n",
    "    flat_label = label.flatten()\n",
    "\n",
    "    for i in range(6):\n",
    "        \n",
    "        # Calculate intersection and union\n",
    "        intersection = np.sum((flat_prediction == i) & (flat_label == i))\n",
    "        union = np.sum((flat_prediction == i) | (flat_label == i))\n",
    "\n",
    "        # Calculate IoU\n",
    "        iou = intersection / (union + 1e-12)\n",
    "\n",
    "        # Store IoU value in dictionary\n",
    "        iou_list.append(iou)\n",
    "    \n",
    "    # calculate overall IoU\n",
    "    intersection = len(np.where(flat_prediction == flat_label)[0])\n",
    "    union = len(flat_label)\n",
    "    overall_iou = intersection / (union + 1e-12)\n",
    "    \n",
    "    return overall_iou,iou_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d6f9ea5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "# get labels for dataloader\n",
    "combined_labels = []\n",
    "combined_labels = np.tile(subset_labels,(len(noise_levels),1,1)) # repeat the array 4 times along the first axis, while keeping the other axes unchanged\n",
    "print(combined_labels.shape)\n",
    "\n",
    "# construct data loader\n",
    "batch_size = 1\n",
    "noise_dataset = CityscapesDataset(combined_images, combined_labels,noise_level=0)\n",
    "noise_loader = DataLoader(noise_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e7eaf389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 256, 256, 3)\n",
      "(400, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "print(combined_images.shape)\n",
    "print(combined_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "029463b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getActivation(name):\n",
    "    # the hook signature\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "model_activations = []\n",
    "overall_ious = []\n",
    "ious_by_class =[]\n",
    "predictions = []\n",
    "\n",
    "for image,label in noise_loader:\n",
    "    activation = {}\n",
    "    h = model_.middle[3].register_forward_hook(getActivation(\"bottleneck\"))\n",
    "    out = model_(image)\n",
    "    prediction = torch.argmax(out, dim=1)\n",
    "    h.remove()\n",
    "    overall_iou,iou_list = IoU_per_class(prediction,label)\n",
    "    overall_ious.append(overall_iou)\n",
    "    ious_by_class.append(iou_list)\n",
    "    prediction = prediction.detach().numpy()[0]\n",
    "    predictions.append(prediction)\n",
    "    instance_activation = activation[\"bottleneck\"]\n",
    "#     print(instance_activation.size())\n",
    "    # need to reshape\n",
    "    instance_activation_reshaped = instance_activation.clone().squeeze(0).reshape(256,1024).numpy()\n",
    "    model_activations.append(instance_activation_reshaped)\n",
    "#     print(instance_activation_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9591102a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_activations = np.array(model_activations)\n",
    "overall_ious = np.array(overall_ious)\n",
    "ious_by_class = np.array(ious_by_class)\n",
    "predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3d87e632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ffb174",
   "metadata": {},
   "source": [
    "Save predictions to paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "13ecd2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_by_noise_level=np.reshape(predictions,(len(noise_levels),sample_number,256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "09a85d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_prediction_folder = \"D:\\zsh\\graduation\\Graduation-project-domain-shift-image-2-image\\dataset\\\\noise_data\\predictions\"\n",
    "prediction_paths = []\n",
    "for i,noisy_predictions in enumerate(predictions_by_noise_level):\n",
    "    noise_level = noise_levels[i]\n",
    "    for j,prediction in enumerate(noisy_predictions):\n",
    "        prediction_object = Image.fromarray(prediction.astype(np.uint8))\n",
    "        # TODO: change the name when use other data\n",
    "        name = str(j)+\".jpeg\"\n",
    "        prediction_path = os.path.join(noisy_prediction_folder,str(noise_level),name)\n",
    "        prediction_path_rel = os.path.relpath(prediction_path, start)\n",
    "        # save image\n",
    "        prediction_object.save(prediction_path)      \n",
    "        \n",
    "        prediction_paths.append(prediction_path_rel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa23ecd",
   "metadata": {},
   "source": [
    "Embedding for activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8cb022dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the activations\n",
    "# TODO: change this shape for pre-trained model\n",
    "activations_2d = np.reshape(model_activations,(len(noise_levels)*sample_number*256,1024)) # 256 is 16x16, the activation \"image\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "53cef6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_50 = PCA(n_components=50)\n",
    "intermediate_pca_embedding = pca_50.fit_transform(activations_2d)\n",
    "activations_tsne_embedding = TSNE(n_components=2).fit_transform(intermediate_pca_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0ea480ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations_tsne_embedding_reshaped = np.reshape(activations_tsne_embedding,(len(noise_levels)*sample_number,256,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "eb95e62d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2c86d7dfeb8>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(activations_tsne_embedding_reshaped[0,:,0],activations_tsne_embedding_reshaped[0,:,1])\n",
    "plt.scatter(activations_tsne_embedding_reshaped[300,:,0],activations_tsne_embedding_reshaped[300,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "912215d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save everything to df\n",
    "\n",
    "# model activations\n",
    "df[\"bottleneck_activations_embedding\"] = activations_tsne_embedding_reshaped.tolist()\n",
    "\n",
    "# prediction path\n",
    "df[\"prediction_path\"] = prediction_paths\n",
    "\n",
    "# overall_iou\n",
    "df[\"overall_iou\"] = overall_ious\n",
    "\n",
    "# iou by class\n",
    "iou_class_df = pd.DataFrame(ious_by_class,columns = [\"other_iou\",\"road_iou\",\"sidewalk_iou\",\"vegetation_iou\",\"sky_iou\",\"car_iou\"])\n",
    "df = pd.concat((df,iou_class_df),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ae9b0008",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rearrange = df[['name','dataset', 'image_path', 'label_path','prediction_path', \n",
    "                       'other_ratio', 'road_ratio', 'sidewalk_ratio','vegetation_ratio', 'sky_ratio', 'car_ratio', \n",
    "                       'simple_tsne_1','simple_tsne_2', 'meaningful_tsne_1', 'meaningful_tsne_2',\"pca_1\",\"pca_2\",\n",
    "       'bottleneck_activations_embedding', 'overall_iou', 'other_iou',\n",
    "       'road_iou', 'sidewalk_iou', 'vegetation_iou', 'sky_iou', 'car_iou']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1b3bb521",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rearrange.to_csv(\"noise_df.csv\",index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
