{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2dbf8e6",
   "metadata": {},
   "source": [
    "This file is based on \"4.27- Link input with activations.ipynb\", with updated data structure (different list structures -> list of tuples). There are also more data that are useful for making visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5be7218",
   "metadata": {},
   "source": [
    "1. Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "dec32cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "\n",
    "# Use IoU instead\n",
    "# import re\n",
    "from scipy.spatial.distance import cdist\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1ae068",
   "metadata": {},
   "source": [
    "2. Decide the procedure by grouping the different columns together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5da86f2",
   "metadata": {},
   "source": [
    "For cityscapes and synthia separately (and then concatenate):\n",
    "\n",
    "* name, image_path, dataset\n",
    "\n",
    "* label_path:\n",
    "    * first get labels (in class format)\n",
    "        * for synthia, need to transform to class format specifically\n",
    "            * for calculating similarity\n",
    "        * also from class to color as well, because use the color of cityscapes\n",
    "            * for saving the label and getting the label path\n",
    "        * get the saved path and save it in a list\n",
    "    \n",
    "* similar_image_paths，similar_IoU_score (Note: called IoU_score originally): finding the most similar masks from another dataset\n",
    "* Generate from os list of images:\n",
    "    * class distribution\n",
    "        * (other_ratio, road_ratio, sidewalk_ratio, vegetation_ratio, sky_ratio, car_ratio)\n",
    "    * embedding of input (originally: tsne_1, tsne_2)\n",
    "        simple dimensionality reduction (simple_tsne_1,simple_tsne_2)\n",
    "        and also the embedding from classification model(meaningful_tsne_1,meaningful_tsne_1)\n",
    "* bottleneck_activations_embedding, prediction_path, and performance\n",
    "    * save output locally, and then add path (prediction_path)\n",
    "    * performance: (other_IoU, road_IoU, sidewalk_IoU, vegetation_IoU, sky_IoU, car_IoU)\n",
    "        **How do you get the IoU per class (check original paper)**\n",
    "\n",
    "**Make sure that the column names are the same as current！！！**\n",
    "\n",
    "**Remember to also save id**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cd5abe",
   "metadata": {},
   "source": [
    "# name, image_path, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8e86aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path of the folders\n",
    "image_path_cityscapes = \"D:\\zsh\\graduation\\Graduation-project-domain-shift-image-2-image\\dataset\\original_cityscapes_inputs\"\n",
    "image_path_synthia = \"D:\\zsh\\graduation\\Graduation-project-domain-shift-image-2-image\\dataset\\SYNTHIA_256\\image\"\n",
    "\n",
    "start = \"D:\\zsh\\graduation\\Graduation-project-domain-shift-image-2-image\"\n",
    "relative_img_path_cityscapes = os.path.relpath(image_path_cityscapes, start)\n",
    "relative_img_path_synthia = os.path.relpath(image_path_synthia, start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "422f5e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_number = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a690a09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cityscapes_names= os.listdir(image_path_cityscapes)\n",
    "random.seed(55)\n",
    "cityscapes_names_sample = random.sample(cityscapes_names,sample_number)\n",
    "\n",
    "synthia_names = os.listdir(image_path_synthia)\n",
    "# randomly select 100 images to load in to numpy array\n",
    "random.seed(55)\n",
    "synthia_names_sample = random.sample(synthia_names,sample_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "484c59f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cityscapes\n",
    "cityscapes_initial_info = []\n",
    "cityscapes_img_sample = []\n",
    "\n",
    "for name in cityscapes_names_sample:\n",
    "    image = Image.open(image_path_cityscapes+\"\\\\\"+name).convert(\"RGB\")\n",
    "    image_path = relative_img_path_cityscapes+\"\\\\\"+name\n",
    "    \n",
    "    cityscapes_img_sample.append(np.array(image))\n",
    "    cityscapes_initial_info.append((name,image_path,\"Cityscapes\"))\n",
    "\n",
    "cityscapes_initial_info = np.array(cityscapes_initial_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e550a33d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['14.jpeg', 'dataset\\\\original_cityscapes_inputs\\\\14.jpeg',\n",
       "        'Cityscapes'],\n",
       "       ['87.jpeg', 'dataset\\\\original_cityscapes_inputs\\\\87.jpeg',\n",
       "        'Cityscapes'],\n",
       "       ['189.jpeg', 'dataset\\\\original_cityscapes_inputs\\\\189.jpeg',\n",
       "        'Cityscapes'],\n",
       "       ['167.jpeg', 'dataset\\\\original_cityscapes_inputs\\\\167.jpeg',\n",
       "        'Cityscapes'],\n",
       "       ['439.jpeg', 'dataset\\\\original_cityscapes_inputs\\\\439.jpeg',\n",
       "        'Cityscapes']], dtype='<U43')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cityscapes_initial_info[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1da2b279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthia\n",
    "\n",
    "synthia_initial_info = []\n",
    "synthia_img_sample = []\n",
    "\n",
    "for name in synthia_names_sample:\n",
    "    image = Image.open(image_path_synthia+\"\\\\\"+name).convert(\"RGB\")\n",
    "    image_path = relative_img_path_synthia+\"\\\\\"+name\n",
    "    synthia_initial_info.append((name, image_path,\"Synthia\"))\n",
    "    synthia_img_sample.append(np.array(image))\n",
    "\n",
    "synthia_initial_info = np.array(synthia_initial_info)\n",
    "synthia_img_sample = np.array(synthia_img_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ab69938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['0001480.png', 'dataset\\\\SYNTHIA_256\\\\image\\\\0001480.png',\n",
       "        'Synthia'],\n",
       "       ['0003215.png', 'dataset\\\\SYNTHIA_256\\\\image\\\\0003215.png',\n",
       "        'Synthia'],\n",
       "       ['0002457.png', 'dataset\\\\SYNTHIA_256\\\\image\\\\0002457.png',\n",
       "        'Synthia'],\n",
       "       ['0004959.png', 'dataset\\\\SYNTHIA_256\\\\image\\\\0004959.png',\n",
       "        'Synthia'],\n",
       "       ['0001305.png', 'dataset\\\\SYNTHIA_256\\\\image\\\\0001305.png',\n",
       "        'Synthia']], dtype='<U37')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthia_initial_info[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f470582",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_initial_info = np.concatenate((cityscapes_initial_info,synthia_initial_info),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "629d2225",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(combined_initial_info,columns = [\"name\", \"image_path\",\"dataset\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d15eda9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>image_path</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.jpeg</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\14.jpeg</td>\n",
       "      <td>Cityscapes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87.jpeg</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\87.jpeg</td>\n",
       "      <td>Cityscapes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>189.jpeg</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\189.jpeg</td>\n",
       "      <td>Cityscapes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>167.jpeg</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\167.jpeg</td>\n",
       "      <td>Cityscapes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>439.jpeg</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\439.jpeg</td>\n",
       "      <td>Cityscapes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       name                                   image_path     dataset\n",
       "0   14.jpeg   dataset\\original_cityscapes_inputs\\14.jpeg  Cityscapes\n",
       "1   87.jpeg   dataset\\original_cityscapes_inputs\\87.jpeg  Cityscapes\n",
       "2  189.jpeg  dataset\\original_cityscapes_inputs\\189.jpeg  Cityscapes\n",
       "3  167.jpeg  dataset\\original_cityscapes_inputs\\167.jpeg  Cityscapes\n",
       "4  439.jpeg  dataset\\original_cityscapes_inputs\\439.jpeg  Cityscapes"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8381f2b",
   "metadata": {},
   "source": [
    "# Synthia color transformation code (for later use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b731aea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthia_colors = [[  0,   0,   0], # void\n",
    "         [70,130, 180], # sky\n",
    "         [70,70,70], # building\n",
    "        [128, 64, 128], # road\n",
    "        [244, 35, 232], # sidewalk\n",
    "         [64,64,128], # fense\n",
    "         [107,142,35], # vegetation\t\n",
    "        [153, 153, 153], # pole\n",
    "        [0, 0, 142], # car\n",
    "        [220, 220, 0],  # traffic sign\n",
    "        [220, 20, 60], # pedestrian\n",
    "        [119, 11, 32], # bicycle\n",
    "        [0, 0, 230], # motorcycle\n",
    "        [250,170,160], # parking-slot\n",
    "        [128,64,64], # road-work\n",
    "        [250,170,30], # traffic light\n",
    "        [152, 251, 152], # terrain\n",
    "        [255, 0, 0], # rider\n",
    "        [0, 0, 70], # truck\n",
    "        [0, 60, 100], # bus\n",
    "        [0, 80, 100], # train\n",
    "        [102, 102, 156]# wall, lanemarking\n",
    "    ]\n",
    "\n",
    "\n",
    "category_map = {\n",
    "    0: 0,\n",
    "    1: 4,\n",
    "    2: 0,\n",
    "    3: 1,\n",
    "    4: 2,\n",
    "    5: 0,\n",
    "    6: 3,\n",
    "    7: 0,\n",
    "    8: 5,\n",
    "    9: 0,\n",
    "    10: 0,\n",
    "    11: 0,\n",
    "    12: 0,\n",
    "    13: 0,\n",
    "    14: 0,\n",
    "    15: 0,\n",
    "    16: 0,\n",
    "    17: 0,\n",
    "    18: 0,\n",
    "    19: 0,\n",
    "    20: 0,\n",
    "    21: 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "84471b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# color to class\n",
    "def color_to_class(label):\n",
    "    # create new empty mask\n",
    "    mask = np.zeros(shape=(label.shape[0], label.shape[1]), dtype = np.int32)\n",
    "    # iterate through two dimensions\n",
    "    for row in range(label.shape[0]):\n",
    "        for col in range(label.shape[1]):\n",
    "            a = label[row, col,:]\n",
    "            # distance between this pixel and the original pixel\n",
    "            d = cdist(np.array([a]),np.array(synthia_colors))\n",
    "            idx = np.argmin(d)\n",
    "            new_idx = category_map[idx]\n",
    "            mask[row, col] = new_idx\n",
    "    mask = np.reshape(mask, (mask.shape[0], mask.shape[1]))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6ce098ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class to color: useful for displaying the similar images later\n",
    "colors = [[  0,   0,   0],\n",
    "          [128, 64, 128],# road\n",
    "          [244, 35, 232], # sidewalk\n",
    "          [107, 142, 35],# vegetation\n",
    "          [70, 130, 180], # sky\n",
    "          [0, 0, 142], # car\n",
    "         ]\n",
    "\n",
    "def class_to_color(labels):\n",
    "    label_colors = np.zeros((256,256,3))\n",
    "    \n",
    "    for i,row in enumerate(labels):\n",
    "        for j,pixel in enumerate(row):\n",
    "            label_colors[i,j] = colors[pixel]\n",
    "    \n",
    "    return label_colors.astype(int)  # make each pixel value an integer to visualize it better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d947ac96",
   "metadata": {},
   "source": [
    "# label_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea294ede",
   "metadata": {},
   "source": [
    "Cityscapes labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b215d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relative paths for cityscapes\n",
    "cityscpaes_save_labels_path = \"D:\\zsh\\graduation\\Graduation-project-domain-shift-image-2-image\\dataset\\cityscapes_labels_sample\"\n",
    "cityscapes_label_folder_relative = os.path.relpath(cityscpaes_save_labels_path, start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1acd4c",
   "metadata": {},
   "source": [
    "**For now: still use the sample of labels. Possibly changing to something else later on**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3a305d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels of cityscapes\n",
    "pickle_file = os.path.join(\"D:\\zsh\\graduation\\Graduation-project-domain-shift-image-2-image\\dataset\\\\5_classes_preprocessed\", \"validation_label_classes.pkl\")\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    Y_test = pickle.load(f)\n",
    "    \n",
    "cityscapes_labels = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9157199d",
   "metadata": {},
   "source": [
    "Generate and save labels for the subset for displaying in the VA system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c69b5465",
   "metadata": {},
   "outputs": [],
   "source": [
    "cityscapes_labels_sample= []\n",
    "cityscapes_label_path_sample = []\n",
    "\n",
    "for name in cityscapes_names_sample:\n",
    "    ind = int(name.split('.')[0])\n",
    "    label = cityscapes_labels[ind]\n",
    "    label_path = cityscapes_label_folder_relative+\"\\\\\"+name\n",
    "    \n",
    "    cityscapes_labels_sample.append(label)\n",
    "    cityscapes_label_path_sample.append(label_path)\n",
    "\n",
    "cityscapes_labels_sample = np.array(cityscapes_labels_sample)\n",
    "cityscapes_label_path_sample = np.array(cityscapes_label_path_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5817f5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(sample_number):\n",
    "    name = cityscapes_names_sample[i]\n",
    "    label = cityscapes_labels_sample[i]\n",
    "    label_color = class_to_color(label)\n",
    "    label_image = Image.fromarray(label_color.astype(np.uint8))\n",
    "    label_image.save(cityscpaes_save_labels_path+\"\\\\\"+name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed06bad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cityscapes_label_path_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc68b11a",
   "metadata": {},
   "source": [
    "Synthia labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1ab440ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthia_label_original_folder = \"D:\\zsh\\graduation\\Graduation-project-domain-shift-image-2-image\\dataset\\SYNTHIA_256\\label-rgb\"\n",
    "synthia_save_labels_path = \"D:\\zsh\\graduation\\Graduation-project-domain-shift-image-2-image\\dataset\\SYNTHIA_256\\\\new_labels\"\n",
    "synthia_label_folder_relative = os.path.relpath(synthia_save_labels_path, start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d21a2d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def transform_masks_synthia(labels):\n",
    "#     masks = []\n",
    "#     for label in labels:\n",
    "#         mask = color_to_class(label)\n",
    "#         masks.append(mask)\n",
    "#     masks = np.array(masks)\n",
    "\n",
    "#     return masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fe9a36",
   "metadata": {},
   "source": [
    "Note: comment out the corresponding lines if loading the synthia_labels_sample.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fed0f620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this usually takes a long time\n",
    "synthia_labels_sample= []\n",
    "synthia_label_path_sample = []\n",
    "\n",
    "for name in synthia_names_sample:\n",
    "    original_label_rgb = Image.open(synthia_label_original_folder+\"\\\\\"+name).convert(\"RGB\")\n",
    "    original_label_rgb = np.array(original_label_rgb)\n",
    "    label = color_to_class(original_label_rgb)\n",
    "    resized_label_rgb = class_to_color(label)\n",
    "    label_path =synthia_label_folder_relative+\"\\\\\"+name\n",
    "    resized_label_rgb = Image.fromarray(resized_label_rgb.astype(np.uint8))\n",
    "    \n",
    "#     save the new labels\n",
    "    resized_label_rgb.save(synthia_save_labels_path+\"\\\\\"+name)\n",
    "    \n",
    "    synthia_labels_sample.append(label)\n",
    "    synthia_label_path_sample.append(label_path)\n",
    "\n",
    "synthia_labels_sample = np.array(synthia_labels_sample)\n",
    "synthia_label_path_sample = np.array(synthia_label_path_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "81b84adb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(synthia_label_path_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59068a3",
   "metadata": {},
   "source": [
    "save the synthia masks in a pickle to save effort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b11d6de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('synthia_labels_sample.pkl', 'wb') as file:\n",
    "      \n",
    "#     # A new file will be created\n",
    "#     pickle.dump(synthia_labels_sample, file)\n",
    "\n",
    "with open('synthia_labels_sample.pkl', 'rb') as f:\n",
    "    synthia_labels_sample = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab2ce25",
   "metadata": {},
   "source": [
    "Combine cityscapes_label_path_sample and synthia_label_path_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "876a8c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_path_sample = np.concatenate((cityscapes_label_path_sample,synthia_label_path_sample),axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2850f14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label_path\"] = label_path_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a4108406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>image_path</th>\n",
       "      <th>dataset</th>\n",
       "      <th>label_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>475.jpeg</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\475.jpeg</td>\n",
       "      <td>Cityscapes</td>\n",
       "      <td>dataset\\cityscapes_labels_sample\\475.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>218.jpeg</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\218.jpeg</td>\n",
       "      <td>Cityscapes</td>\n",
       "      <td>dataset\\cityscapes_labels_sample\\218.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>235.jpeg</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\235.jpeg</td>\n",
       "      <td>Cityscapes</td>\n",
       "      <td>dataset\\cityscapes_labels_sample\\235.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>242.jpeg</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\242.jpeg</td>\n",
       "      <td>Cityscapes</td>\n",
       "      <td>dataset\\cityscapes_labels_sample\\242.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>222.jpeg</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\222.jpeg</td>\n",
       "      <td>Cityscapes</td>\n",
       "      <td>dataset\\cityscapes_labels_sample\\222.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0001480.png</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0001480.png</td>\n",
       "      <td>Synthia</td>\n",
       "      <td>dataset\\SYNTHIA_256\\new_labels\\0001480.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0003215.png</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0003215.png</td>\n",
       "      <td>Synthia</td>\n",
       "      <td>dataset\\SYNTHIA_256\\new_labels\\0003215.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0002457.png</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0002457.png</td>\n",
       "      <td>Synthia</td>\n",
       "      <td>dataset\\SYNTHIA_256\\new_labels\\0002457.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0004959.png</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0004959.png</td>\n",
       "      <td>Synthia</td>\n",
       "      <td>dataset\\SYNTHIA_256\\new_labels\\0004959.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0001305.png</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0001305.png</td>\n",
       "      <td>Synthia</td>\n",
       "      <td>dataset\\SYNTHIA_256\\new_labels\\0001305.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            name                                   image_path     dataset  \\\n",
       "95      475.jpeg  dataset\\original_cityscapes_inputs\\475.jpeg  Cityscapes   \n",
       "96      218.jpeg  dataset\\original_cityscapes_inputs\\218.jpeg  Cityscapes   \n",
       "97      235.jpeg  dataset\\original_cityscapes_inputs\\235.jpeg  Cityscapes   \n",
       "98      242.jpeg  dataset\\original_cityscapes_inputs\\242.jpeg  Cityscapes   \n",
       "99      222.jpeg  dataset\\original_cityscapes_inputs\\222.jpeg  Cityscapes   \n",
       "100  0001480.png        dataset\\SYNTHIA_256\\image\\0001480.png     Synthia   \n",
       "101  0003215.png        dataset\\SYNTHIA_256\\image\\0003215.png     Synthia   \n",
       "102  0002457.png        dataset\\SYNTHIA_256\\image\\0002457.png     Synthia   \n",
       "103  0004959.png        dataset\\SYNTHIA_256\\image\\0004959.png     Synthia   \n",
       "104  0001305.png        dataset\\SYNTHIA_256\\image\\0001305.png     Synthia   \n",
       "\n",
       "                                     label_path  \n",
       "95    dataset\\cityscapes_labels_sample\\475.jpeg  \n",
       "96    dataset\\cityscapes_labels_sample\\218.jpeg  \n",
       "97    dataset\\cityscapes_labels_sample\\235.jpeg  \n",
       "98    dataset\\cityscapes_labels_sample\\242.jpeg  \n",
       "99    dataset\\cityscapes_labels_sample\\222.jpeg  \n",
       "100  dataset\\SYNTHIA_256\\new_labels\\0001480.png  \n",
       "101  dataset\\SYNTHIA_256\\new_labels\\0003215.png  \n",
       "102  dataset\\SYNTHIA_256\\new_labels\\0002457.png  \n",
       "103  dataset\\SYNTHIA_256\\new_labels\\0004959.png  \n",
       "104  dataset\\SYNTHIA_256\\new_labels\\0001305.png  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[95:105]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44962d55",
   "metadata": {},
   "source": [
    "# similar_image_paths，similar_IoU_score \n",
    "(Note: called IoU_score originally): finding the most similar masks from another dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "932cd88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_mask(instance, data,mode = \"IoU\"):\n",
    "#     nodes = np.asarray(nodes)\n",
    "    # Euclidean distance calculation\n",
    "    if mode == \"IoU\":\n",
    "        iou_list = []\n",
    "        for new_instance in data:\n",
    "#             the original IoU methods were not good because when both class is 0, the intersection won't count it\n",
    "#             intersection = np.logical_and(instance, new_instance)\n",
    "#             union = np.logical_or(instance, new_instance)\n",
    "            intersection = len(np.where(instance == new_instance)[0])\n",
    "            union = instance.shape[0]*instance.shape[1]\n",
    "#             print(intersection)\n",
    "#             print(union)\n",
    "            iou_score = intersection / union\n",
    "            iou_list.append(iou_score)\n",
    "        iou_arr = np.array(iou_list)\n",
    "        best_index = np.argmax(iou_arr)\n",
    "        best_score = iou_arr[best_index]\n",
    "    \n",
    "    return best_index, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ab963754",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'arra'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-af70f1ca631f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mcityscapes_similar_image_paths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcityscapes_similar_image_paths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mcityscapes_similar_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marra\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcityscapes_similar_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'arra'"
     ]
    }
   ],
   "source": [
    "cityscapes_similar_image_paths = []\n",
    "cityscapes_similar_scores = []\n",
    "\n",
    "for label in cityscapes_labels_sample:\n",
    "    image_index_synthia,similar_score = most_similar_mask(label,synthia_labels_sample)\n",
    "    similar_image_path = synthia_initial_info[image_index_synthia][1]\n",
    "    \n",
    "    cityscapes_similar_image_paths.append(similar_image_path)\n",
    "    cityscapes_similar_scores.append(similar_score)\n",
    "\n",
    "cityscapes_similar_image_paths = np.array(cityscapes_similar_image_paths)\n",
    "cityscapes_similar_scores = np.array(cityscapes_similar_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c971e619",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthia_similar_image_paths = []\n",
    "synthia_similar_scores = []\n",
    "\n",
    "for label in synthia_labels_sample:\n",
    "    image_index_cityscapes,similar_score = most_similar_mask(label,cityscapes_labels_sample)\n",
    "    similar_image_path = cityscapes_initial_info[image_index_cityscapes][1]\n",
    "    \n",
    "    synthia_similar_image_paths.append(similar_image_path)\n",
    "    synthia_similar_scores.append(similar_score)\n",
    "\n",
    "synthia_similar_image_paths = np.array(synthia_similar_image_paths)\n",
    "synthia_similar_scores = np.array(synthia_similar_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ba513e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_image_paths =  np.concatenate((cityscapes_similar_image_paths,synthia_similar_image_paths),axis = 0)\n",
    "similar_scores = np.concatenate((cityscapes_similar_scores,synthia_similar_scores),axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ae210e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"similar_image_paths\"] = similar_image_paths\n",
    "df[\"similar_IoU_score\"]=similar_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ef806577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>image_path</th>\n",
       "      <th>dataset</th>\n",
       "      <th>label_path</th>\n",
       "      <th>similar_image_paths</th>\n",
       "      <th>similar_IoU_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>475.jpeg</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\475.jpeg</td>\n",
       "      <td>Cityscapes</td>\n",
       "      <td>dataset\\cityscapes_labels_sample\\475.jpeg</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0004421.png</td>\n",
       "      <td>0.629333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>218.jpeg</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\218.jpeg</td>\n",
       "      <td>Cityscapes</td>\n",
       "      <td>dataset\\cityscapes_labels_sample\\218.jpeg</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0002688.png</td>\n",
       "      <td>0.541000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>235.jpeg</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\235.jpeg</td>\n",
       "      <td>Cityscapes</td>\n",
       "      <td>dataset\\cityscapes_labels_sample\\235.jpeg</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0005321.png</td>\n",
       "      <td>0.609619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>242.jpeg</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\242.jpeg</td>\n",
       "      <td>Cityscapes</td>\n",
       "      <td>dataset\\cityscapes_labels_sample\\242.jpeg</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0005321.png</td>\n",
       "      <td>0.502472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>222.jpeg</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\222.jpeg</td>\n",
       "      <td>Cityscapes</td>\n",
       "      <td>dataset\\cityscapes_labels_sample\\222.jpeg</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0004607.png</td>\n",
       "      <td>0.710312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0001480.png</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0001480.png</td>\n",
       "      <td>Synthia</td>\n",
       "      <td>dataset\\SYNTHIA_256\\new_labels\\0001480.png</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\237.jpeg</td>\n",
       "      <td>0.435898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0003215.png</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0003215.png</td>\n",
       "      <td>Synthia</td>\n",
       "      <td>dataset\\SYNTHIA_256\\new_labels\\0003215.png</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\188.jpeg</td>\n",
       "      <td>0.492340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0002457.png</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0002457.png</td>\n",
       "      <td>Synthia</td>\n",
       "      <td>dataset\\SYNTHIA_256\\new_labels\\0002457.png</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\147.jpeg</td>\n",
       "      <td>0.487686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0004959.png</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0004959.png</td>\n",
       "      <td>Synthia</td>\n",
       "      <td>dataset\\SYNTHIA_256\\new_labels\\0004959.png</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\101.jpeg</td>\n",
       "      <td>0.587204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0001305.png</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0001305.png</td>\n",
       "      <td>Synthia</td>\n",
       "      <td>dataset\\SYNTHIA_256\\new_labels\\0001305.png</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\423.jpeg</td>\n",
       "      <td>0.536331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            name                                   image_path     dataset  \\\n",
       "95      475.jpeg  dataset\\original_cityscapes_inputs\\475.jpeg  Cityscapes   \n",
       "96      218.jpeg  dataset\\original_cityscapes_inputs\\218.jpeg  Cityscapes   \n",
       "97      235.jpeg  dataset\\original_cityscapes_inputs\\235.jpeg  Cityscapes   \n",
       "98      242.jpeg  dataset\\original_cityscapes_inputs\\242.jpeg  Cityscapes   \n",
       "99      222.jpeg  dataset\\original_cityscapes_inputs\\222.jpeg  Cityscapes   \n",
       "100  0001480.png        dataset\\SYNTHIA_256\\image\\0001480.png     Synthia   \n",
       "101  0003215.png        dataset\\SYNTHIA_256\\image\\0003215.png     Synthia   \n",
       "102  0002457.png        dataset\\SYNTHIA_256\\image\\0002457.png     Synthia   \n",
       "103  0004959.png        dataset\\SYNTHIA_256\\image\\0004959.png     Synthia   \n",
       "104  0001305.png        dataset\\SYNTHIA_256\\image\\0001305.png     Synthia   \n",
       "\n",
       "                                     label_path  \\\n",
       "95    dataset\\cityscapes_labels_sample\\475.jpeg   \n",
       "96    dataset\\cityscapes_labels_sample\\218.jpeg   \n",
       "97    dataset\\cityscapes_labels_sample\\235.jpeg   \n",
       "98    dataset\\cityscapes_labels_sample\\242.jpeg   \n",
       "99    dataset\\cityscapes_labels_sample\\222.jpeg   \n",
       "100  dataset\\SYNTHIA_256\\new_labels\\0001480.png   \n",
       "101  dataset\\SYNTHIA_256\\new_labels\\0003215.png   \n",
       "102  dataset\\SYNTHIA_256\\new_labels\\0002457.png   \n",
       "103  dataset\\SYNTHIA_256\\new_labels\\0004959.png   \n",
       "104  dataset\\SYNTHIA_256\\new_labels\\0001305.png   \n",
       "\n",
       "                             similar_image_paths  similar_IoU_score  \n",
       "95         dataset\\SYNTHIA_256\\image\\0004421.png           0.629333  \n",
       "96         dataset\\SYNTHIA_256\\image\\0002688.png           0.541000  \n",
       "97         dataset\\SYNTHIA_256\\image\\0005321.png           0.609619  \n",
       "98         dataset\\SYNTHIA_256\\image\\0005321.png           0.502472  \n",
       "99         dataset\\SYNTHIA_256\\image\\0004607.png           0.710312  \n",
       "100  dataset\\original_cityscapes_inputs\\237.jpeg           0.435898  \n",
       "101  dataset\\original_cityscapes_inputs\\188.jpeg           0.492340  \n",
       "102  dataset\\original_cityscapes_inputs\\147.jpeg           0.487686  \n",
       "103  dataset\\original_cityscapes_inputs\\101.jpeg           0.587204  \n",
       "104  dataset\\original_cityscapes_inputs\\423.jpeg           0.536331  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[95:105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "81749cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"system_df_full.csv\",index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53461af6",
   "metadata": {},
   "source": [
    "# generate from list of images and labels\n",
    "\n",
    "* class distribution\n",
    "    * (other_ratio, road_ratio, sidewalk_ratio, vegetation_ratio, sky_ratio, car_ratio)\n",
    "* embedding of input (originally: tsne_1, tsne_2)\n",
    "    * simple dimensionality reduction (simple_tsne_1,simple_tsne_2)\n",
    "    * and also the embedding from classification model(meaningful_tsne_1,meaningful_tsne_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8946451b",
   "metadata": {},
   "source": [
    "Load images to data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ac00771b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityscapesDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, images, labels, noise_level = 0):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.noise_level = noise_level\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        label = self.labels[index]\n",
    "        if self.noise_level!=0:\n",
    "            image = image+(self.noise_level*np.random.normal(0, (image.max() - image.min()), image.shape)).astype(\"uint8\") # (mean, sigma, image_shape)\n",
    "        image = self.transform(image)\n",
    "        label = torch.from_numpy(label).long()\n",
    "        return image, label\n",
    "        \n",
    "    def transform(self, image):\n",
    "        transform_ops = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)) # normalize to control the \"dynamic range\" of activations of different layers\n",
    "        ])\n",
    "        return transform_ops(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8594fd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = \"D:\\zsh\\graduation\\\\Graduation-project-domain-shift-image-2-image\"\n",
    "# cityscapes_label_path = \"dataset\\\\cityscapes_labels_100\"\n",
    "cityscapes_images = []\n",
    "cityscapes_labels = []\n",
    "synthia_images = []\n",
    "synthia_labels = []\n",
    "\n",
    "for index,path in enumerate(df[\"image_path\"]):\n",
    "    full_path = os.path.join(repo_path,path)\n",
    "    dataset = df[\"dataset\"].iloc[index]\n",
    "    # parse to get the name of image\n",
    "    name = os.path.split(path)[1]\n",
    "    label_index = int(name.split(\".\")[0])\n",
    "    if dataset == \"Cityscapes\":\n",
    "        # image\n",
    "        img = Image.open(full_path)\n",
    "        img = np.array(img)\n",
    "        # label\n",
    "        label = Y_test[label_index]\n",
    "        label = np.array(label)\n",
    "        cityscapes_images.append(img)\n",
    "        cityscapes_labels.append(label)\n",
    "    elif dataset == \"Synthia\":\n",
    "        # image\n",
    "        img = Image.open(full_path)\n",
    "        img = np.array(img)\n",
    "        # label\n",
    "        label = synthia_labels_sample[index-100]\n",
    "        label = np.array(label)\n",
    "        synthia_images.append(img)\n",
    "        synthia_labels.append(label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "89fe7633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 256, 256, 3)\n",
      "(100, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "cityscapes_images = np.array(cityscapes_images)\n",
    "cityscapes_labels = np.array(cityscapes_labels)\n",
    "synthia_images = np.array(synthia_images)\n",
    "synthia_labels = np.array(synthia_labels)\n",
    "\n",
    "print(synthia_images.shape)\n",
    "print(synthia_images.shape)\n",
    "\n",
    "# load to Dataset class and DataLoader\n",
    "batch_size = 1\n",
    "cityscapes_dataset = CityscapesDataset(cityscapes_images, cityscapes_labels,noise_level=0)\n",
    "cityscapes_loader = DataLoader(cityscapes_dataset, batch_size=batch_size)\n",
    "\n",
    "synthia_dataset = CityscapesDataset(synthia_images, synthia_labels,noise_level=0)\n",
    "synthia_loader = DataLoader(synthia_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec3bcd5",
   "metadata": {},
   "source": [
    "**class distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "849db32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_distribution(labels):\n",
    "    class_dist = []\n",
    "    for label in labels:\n",
    "        class_for_label = []\n",
    "        for i in range(6):\n",
    "            element_count = np.count_nonzero(label==i)\n",
    "            class_for_label.append(element_count/(256*256))\n",
    "        class_dist.append(class_for_label)\n",
    "        class_for_label= class_for_label# change to another size if the image size is changed\n",
    "    class_dist = np.array(class_dist)\n",
    "    return class_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c2e257d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cityscpaes_class_dist = get_class_distribution(cityscapes_labels)\n",
    "# synthia_labels\n",
    "synthia_class_dist = get_class_distribution(synthia_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b42c61e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 6)\n",
      "(100, 6)\n"
     ]
    }
   ],
   "source": [
    "print(cityscpaes_class_dist.shape)\n",
    "print(synthia_class_dist.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7a4ee2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dist = np.concatenate((cityscpaes_class_dist,synthia_class_dist),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "bc1983ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>other_ratio</th>\n",
       "      <th>road_ratio</th>\n",
       "      <th>sidewalk_ratio</th>\n",
       "      <th>vegetation_ratio</th>\n",
       "      <th>sky_ratio</th>\n",
       "      <th>car_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.268173</td>\n",
       "      <td>0.380219</td>\n",
       "      <td>0.054855</td>\n",
       "      <td>0.259460</td>\n",
       "      <td>0.026566</td>\n",
       "      <td>0.010727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.390427</td>\n",
       "      <td>0.322311</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.128143</td>\n",
       "      <td>0.004623</td>\n",
       "      <td>0.154495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.615875</td>\n",
       "      <td>0.264603</td>\n",
       "      <td>0.010590</td>\n",
       "      <td>0.037872</td>\n",
       "      <td>0.023697</td>\n",
       "      <td>0.047363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.235931</td>\n",
       "      <td>0.296295</td>\n",
       "      <td>0.021698</td>\n",
       "      <td>0.421646</td>\n",
       "      <td>0.023544</td>\n",
       "      <td>0.000885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.501968</td>\n",
       "      <td>0.312622</td>\n",
       "      <td>0.026047</td>\n",
       "      <td>0.157547</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.001785</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   other_ratio  road_ratio  sidewalk_ratio  vegetation_ratio  sky_ratio  \\\n",
       "0     0.268173    0.380219        0.054855          0.259460   0.026566   \n",
       "1     0.390427    0.322311        0.000000          0.128143   0.004623   \n",
       "2     0.615875    0.264603        0.010590          0.037872   0.023697   \n",
       "3     0.235931    0.296295        0.021698          0.421646   0.023544   \n",
       "4     0.501968    0.312622        0.026047          0.157547   0.000031   \n",
       "\n",
       "   car_ratio  \n",
       "0   0.010727  \n",
       "1   0.154495  \n",
       "2   0.047363  \n",
       "3   0.000885  \n",
       "4   0.001785  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_dist_df = pd.DataFrame(class_dist,columns=[\"other_ratio\",\"road_ratio\",\"sidewalk_ratio\",\"vegetation_ratio\",\"sky_ratio\",\"car_ratio\"])\n",
    "class_dist_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "920d745f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.concat((df,class_dist_df),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9b8b531d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.to_csv(\"system_df_full.csv\",index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119f5080",
   "metadata": {},
   "source": [
    "**embedding of input**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df56adca",
   "metadata": {},
   "source": [
    "* simple dimensionality reduction (simple_tsne_1,simple_tsne_2)\n",
    "* and also the embedding from classification model(meaningful_tsne_1,meaningful_tsne_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "837d803e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple dimensionality reduction\n",
    "combined_images = np.concatenate((cityscapes_images,synthia_images),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7b68364c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7971421033590691\n"
     ]
    }
   ],
   "source": [
    "simple_combined_embedding =np.reshape(combined_images,(len(combined_images), 256*256*3))\n",
    "\n",
    "pca_50 = PCA(n_components=50)\n",
    "pca_embedding = pca_50.fit_transform(simple_combined_embedding)\n",
    "print(np.sum(pca_50.explained_variance_ratio_))\n",
    "tsne = TSNE()\n",
    "simple_tsne_embedding = tsne.fit_transform(pca_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "45ea6e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\zsh\\graduation\\ViTs-vs-CNNs\n"
     ]
    }
   ],
   "source": [
    "cd D:\\zsh\\graduation\\ViTs-vs-CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "51a09a3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import global_val\n",
    "\n",
    "from models.ghost_bn import GhostBN2D_ADV\n",
    "from models.advresnet_gbn_gelu import Affine\n",
    "import models.advresnet_gbn_gelu as advres\n",
    "from main_adv_res import EightBN\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "classifier_model = advres.__dict__[\"resnet50\"](norm_layer = EightBN)\n",
    "weights_path = \"D:\\zsh\\graduation\\Graduation-project-domain-shift-image-2-image\\models\\\\advres50_gelu.pth\"\n",
    "weight_dict = torch.load(weights_path,map_location=device)[\"model\"]\n",
    "\n",
    "classifier_model.load_state_dict(weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "27329cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\zsh\\graduation\\Graduation-project-domain-shift-image-2-image\n"
     ]
    }
   ],
   "source": [
    "cd D:\\zsh\\graduation\\Graduation-project-domain-shift-image-2-image\\jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "5820fe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_representation_for_image(image,label):\n",
    "    # resize the image and transform it to tensor # [256,256,3] -> [224,224,3] \n",
    "    resized_image = resize(image, (224, 224,3))\n",
    "    resized_image_tensor = torch.from_numpy(resized_image)\n",
    "    resized_image_tensor = torch.permute(resized_image_tensor, (2, 0, 1))\n",
    "    resized_image_tensor = resized_image_tensor[None,:] # [3,224,224] -> [1,3,224,224] (because model takes 4D input)\n",
    "    \n",
    "    # dictionary for the activations\n",
    "    activations = {}\n",
    "\n",
    "    def get_activations(name):\n",
    "        def hook(model, input, output):\n",
    "            activations[name] = output.detach()\n",
    "        return hook\n",
    "\n",
    "    # use the output of (avgpool) layer because it is the same as the input of last layer\n",
    "    h = classifier_model.avgpool.register_forward_hook(get_activations(\"input_last_layer\"))\n",
    "\n",
    "    classifier_model.eval()\n",
    "    classifier_model.sing=True\n",
    "    classifier_model.training=False\n",
    "    out = classifier_model(resized_image_tensor.float(),label)\n",
    "    \n",
    "    # remove the hook\n",
    "    h.remove()\n",
    "\n",
    "    first_part_embedding = torch.flatten(activations[\"input_last_layer\"][0])\n",
    "    # the second part of the embedding is size of each class\n",
    "    #  (this section of code is overlapping with generating class distribution, improve this for a better result)   \n",
    "#     second_part_embedding=[]\n",
    "#     for i in range(6):\n",
    "#         class_size = np.count_nonzero(label == i)\n",
    "#         second_part_embedding.append(class_size)\n",
    "#     embedding = np.concatenate((first_part_embedding.numpy(),np.array(second_part_embedding)),axis = 0)\n",
    "    \n",
    "    return first_part_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f79ded9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_list = []\n",
    "for i, image in enumerate(combined_images): # i is not used, could remove the enumerate\n",
    "    first_part_embedding = get_representation_for_image(image,\"__\") # y_label is never used, so just use replacement\n",
    "    second_part_embedding = class_dist[i]\n",
    "    embedding = np.concatenate((first_part_embedding.numpy(),np.array(second_part_embedding)),axis = 0)\n",
    "    embedding_list.append(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "80d6ac09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8664894254259768\n"
     ]
    }
   ],
   "source": [
    "embedding_arr = np.array(embedding_list)\n",
    "pca_50 = PCA(n_components=50)\n",
    "pca_embedding = pca_50.fit_transform(embedding_arr)\n",
    "print(np.sum(pca_50.explained_variance_ratio_))\n",
    "# pca = PCA(n_components=2)\n",
    "tsne = TSNE()\n",
    "meaningful_tsne_embedding = tsne.fit_transform(pca_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "ea83b89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_tsne_embedding = np.concatenate((simple_tsne_embedding,meaningful_tsne_embedding),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "b6ba4c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input_embedding = pd.DataFrame(full_tsne_embedding, columns=[\"simple_tsne_1\",\"simple_tsne_2\",\"meaningful_tsne_1\",\"meaningful_tsne_2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "1a85f126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>image_path</th>\n",
       "      <th>dataset</th>\n",
       "      <th>label_path</th>\n",
       "      <th>similar_image_paths</th>\n",
       "      <th>similar_IoU_score</th>\n",
       "      <th>other_ratio</th>\n",
       "      <th>road_ratio</th>\n",
       "      <th>sidewalk_ratio</th>\n",
       "      <th>vegetation_ratio</th>\n",
       "      <th>sky_ratio</th>\n",
       "      <th>car_ratio</th>\n",
       "      <th>simple_tsne_1</th>\n",
       "      <th>simple_tsne_2</th>\n",
       "      <th>meaningful_tsne_1</th>\n",
       "      <th>meaningful_tsne_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.jpeg</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\14.jpeg</td>\n",
       "      <td>Cityscapes</td>\n",
       "      <td>dataset\\cityscapes_labels_sample\\14.jpeg</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0002688.png</td>\n",
       "      <td>0.475754</td>\n",
       "      <td>0.268173</td>\n",
       "      <td>0.380219</td>\n",
       "      <td>0.054855</td>\n",
       "      <td>0.259460</td>\n",
       "      <td>0.026566</td>\n",
       "      <td>0.010727</td>\n",
       "      <td>2.444247</td>\n",
       "      <td>-4.093524</td>\n",
       "      <td>6.933020</td>\n",
       "      <td>7.210295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87.jpeg</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\87.jpeg</td>\n",
       "      <td>Cityscapes</td>\n",
       "      <td>dataset\\cityscapes_labels_sample\\87.jpeg</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0002688.png</td>\n",
       "      <td>0.480301</td>\n",
       "      <td>0.390427</td>\n",
       "      <td>0.322311</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.128143</td>\n",
       "      <td>0.004623</td>\n",
       "      <td>0.154495</td>\n",
       "      <td>-1.070811</td>\n",
       "      <td>4.178267</td>\n",
       "      <td>4.218904</td>\n",
       "      <td>12.409754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>189.jpeg</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\189.jpeg</td>\n",
       "      <td>Cityscapes</td>\n",
       "      <td>dataset\\cityscapes_labels_sample\\189.jpeg</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0004607.png</td>\n",
       "      <td>0.653656</td>\n",
       "      <td>0.615875</td>\n",
       "      <td>0.264603</td>\n",
       "      <td>0.010590</td>\n",
       "      <td>0.037872</td>\n",
       "      <td>0.023697</td>\n",
       "      <td>0.047363</td>\n",
       "      <td>-3.867693</td>\n",
       "      <td>1.602131</td>\n",
       "      <td>-1.544376</td>\n",
       "      <td>10.230064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>167.jpeg</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\167.jpeg</td>\n",
       "      <td>Cityscapes</td>\n",
       "      <td>dataset\\cityscapes_labels_sample\\167.jpeg</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0005291.png</td>\n",
       "      <td>0.453262</td>\n",
       "      <td>0.235931</td>\n",
       "      <td>0.296295</td>\n",
       "      <td>0.021698</td>\n",
       "      <td>0.421646</td>\n",
       "      <td>0.023544</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>0.109568</td>\n",
       "      <td>-2.603335</td>\n",
       "      <td>3.992030</td>\n",
       "      <td>5.020967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>439.jpeg</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\439.jpeg</td>\n",
       "      <td>Cityscapes</td>\n",
       "      <td>dataset\\cityscapes_labels_sample\\439.jpeg</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0002688.png</td>\n",
       "      <td>0.522873</td>\n",
       "      <td>0.501968</td>\n",
       "      <td>0.312622</td>\n",
       "      <td>0.026047</td>\n",
       "      <td>0.157547</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.001785</td>\n",
       "      <td>3.045528</td>\n",
       "      <td>-4.438342</td>\n",
       "      <td>8.317305</td>\n",
       "      <td>5.757763</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       name                                   image_path     dataset  \\\n",
       "0   14.jpeg   dataset\\original_cityscapes_inputs\\14.jpeg  Cityscapes   \n",
       "1   87.jpeg   dataset\\original_cityscapes_inputs\\87.jpeg  Cityscapes   \n",
       "2  189.jpeg  dataset\\original_cityscapes_inputs\\189.jpeg  Cityscapes   \n",
       "3  167.jpeg  dataset\\original_cityscapes_inputs\\167.jpeg  Cityscapes   \n",
       "4  439.jpeg  dataset\\original_cityscapes_inputs\\439.jpeg  Cityscapes   \n",
       "\n",
       "                                  label_path  \\\n",
       "0   dataset\\cityscapes_labels_sample\\14.jpeg   \n",
       "1   dataset\\cityscapes_labels_sample\\87.jpeg   \n",
       "2  dataset\\cityscapes_labels_sample\\189.jpeg   \n",
       "3  dataset\\cityscapes_labels_sample\\167.jpeg   \n",
       "4  dataset\\cityscapes_labels_sample\\439.jpeg   \n",
       "\n",
       "                     similar_image_paths  similar_IoU_score  other_ratio  \\\n",
       "0  dataset\\SYNTHIA_256\\image\\0002688.png           0.475754     0.268173   \n",
       "1  dataset\\SYNTHIA_256\\image\\0002688.png           0.480301     0.390427   \n",
       "2  dataset\\SYNTHIA_256\\image\\0004607.png           0.653656     0.615875   \n",
       "3  dataset\\SYNTHIA_256\\image\\0005291.png           0.453262     0.235931   \n",
       "4  dataset\\SYNTHIA_256\\image\\0002688.png           0.522873     0.501968   \n",
       "\n",
       "   road_ratio  sidewalk_ratio  vegetation_ratio  sky_ratio  car_ratio  \\\n",
       "0    0.380219        0.054855          0.259460   0.026566   0.010727   \n",
       "1    0.322311        0.000000          0.128143   0.004623   0.154495   \n",
       "2    0.264603        0.010590          0.037872   0.023697   0.047363   \n",
       "3    0.296295        0.021698          0.421646   0.023544   0.000885   \n",
       "4    0.312622        0.026047          0.157547   0.000031   0.001785   \n",
       "\n",
       "   simple_tsne_1  simple_tsne_2  meaningful_tsne_1  meaningful_tsne_2  \n",
       "0       2.444247      -4.093524           6.933020           7.210295  \n",
       "1      -1.070811       4.178267           4.218904          12.409754  \n",
       "2      -3.867693       1.602131          -1.544376          10.230064  \n",
       "3       0.109568      -2.603335           3.992030           5.020967  \n",
       "4       3.045528      -4.438342           8.317305           5.757763  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full = pd.concat((df_full,df_input_embedding),axis = 1)\n",
    "df_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "1c71bf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.to_csv(\"system_df_full.csv\",index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d7f6b7",
   "metadata": {},
   "source": [
    "#  bottleneck_activations_embedding, prediction_path, and performance\n",
    "* save output locally, and then add path (prediction_path)\n",
    "* performance: (other_IoU, road_IoU, sidewalk_IoU, vegetation_IoU, sky_IoU, car_IoU)\n",
    "    **How do you get the IoU per class (check original paper)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461e99b0",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "e5a8e1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes):\n",
    "        super(UNet, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.contracting_11 = self.conv_block(in_channels=3, out_channels=64)\n",
    "        self.contracting_12 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.contracting_21 = self.conv_block(in_channels=64, out_channels=128)\n",
    "        self.contracting_22 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.contracting_31 = self.conv_block(in_channels=128, out_channels=256)\n",
    "        self.contracting_32 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.contracting_41 = self.conv_block(in_channels=256, out_channels=512)\n",
    "        self.contracting_42 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.middle = self.conv_block(in_channels=512, out_channels=1024)\n",
    "        self.expansive_11 = nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.expansive_12 = self.conv_block(in_channels=1024, out_channels=512)\n",
    "        self.expansive_21 = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.expansive_22 = self.conv_block(in_channels=512, out_channels=256)\n",
    "        self.expansive_31 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.expansive_32 = self.conv_block(in_channels=256, out_channels=128)\n",
    "        self.expansive_41 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.expansive_42 = self.conv_block(in_channels=128, out_channels=64)\n",
    "        self.output = nn.Conv2d(in_channels=64, out_channels=num_classes, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        block = nn.Sequential(nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.BatchNorm2d(num_features=out_channels),\n",
    "                                    nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.BatchNorm2d(num_features=out_channels))\n",
    "        return block\n",
    "    \n",
    "    def forward(self, X):\n",
    "        contracting_11_out = self.contracting_11(X) # [-1, 64, 256, 256]\n",
    "        contracting_12_out = self.contracting_12(contracting_11_out) # [-1, 64, 128, 128]\n",
    "        contracting_21_out = self.contracting_21(contracting_12_out) # [-1, 128, 128, 128]\n",
    "        contracting_22_out = self.contracting_22(contracting_21_out) # [-1, 128, 64, 64]\n",
    "        contracting_31_out = self.contracting_31(contracting_22_out) # [-1, 256, 64, 64]\n",
    "        contracting_32_out = self.contracting_32(contracting_31_out) # [-1, 256, 32, 32]\n",
    "        contracting_41_out = self.contracting_41(contracting_32_out) # [-1, 512, 32, 32]\n",
    "        contracting_42_out = self.contracting_42(contracting_41_out) # [-1, 512, 16, 16]\n",
    "        middle_out = self.middle(contracting_42_out) # [-1, 1024, 16, 16]\n",
    "        expansive_11_out = self.expansive_11(middle_out) # [-1, 512, 32, 32]\n",
    "        expansive_12_out = self.expansive_12(torch.cat((expansive_11_out, contracting_41_out), dim=1)) # [-1, 1024, 32, 32] -> [-1, 512, 32, 32]\n",
    "        expansive_21_out = self.expansive_21(expansive_12_out) # [-1, 256, 64, 64]\n",
    "        expansive_22_out = self.expansive_22(torch.cat((expansive_21_out, contracting_31_out), dim=1)) # [-1, 512, 64, 64] -> [-1, 256, 64, 64]\n",
    "        expansive_31_out = self.expansive_31(expansive_22_out) # [-1, 128, 128, 128]\n",
    "        expansive_32_out = self.expansive_32(torch.cat((expansive_31_out, contracting_21_out), dim=1)) # [-1, 256, 128, 128] -> [-1, 128, 128, 128]\n",
    "        expansive_41_out = self.expansive_41(expansive_32_out) # [-1, 64, 256, 256]\n",
    "        expansive_42_out = self.expansive_42(torch.cat((expansive_41_out, contracting_11_out), dim=1)) # [-1, 128, 256, 256] -> [-1, 64, 256, 256]\n",
    "        output_out = self.output(expansive_42_out) # [-1, num_classes, 256, 256]\n",
    "        return output_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ca580983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"D:\\zsh\\graduation\\Graduation-project-domain-shift-image-2-image\\models\\\\5-classes-U-Net-2023-03-09.pth\"\n",
    "\n",
    "device = torch.device('cpu')\n",
    "model_ = UNet(num_classes=6).to(device)\n",
    "model_.load_state_dict(torch.load(model_path,map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "209b7e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IoU_per_class(prediction,label):\n",
    "    prediction = prediction.detach().numpy()\n",
    "    label = label.detach().numpy()\n",
    "    # Loop over each class\n",
    "    iou_list = []\n",
    "    # Flatten label and class arrays\n",
    "    flat_prediction = prediction.flatten()\n",
    "    flat_label = label.flatten()\n",
    "\n",
    "    for i in range(6):\n",
    "        \n",
    "        # Calculate intersection and union\n",
    "        intersection = np.sum((flat_prediction == i) & (flat_label == i))\n",
    "        union = np.sum((flat_prediction == i) | (flat_label == i))\n",
    "\n",
    "        # Calculate IoU\n",
    "        iou = intersection / (union + 1e-12)\n",
    "\n",
    "        # Store IoU value in dictionary\n",
    "        iou_list.append(iou)\n",
    "    \n",
    "    # calculate overall IoU\n",
    "    intersection = len(np.where(flat_prediction == flat_label)[0])\n",
    "    union = len(flat_label)\n",
    "    overall_iou = intersection / (union + 1e-12)\n",
    "    \n",
    "    return overall_iou,iou_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "d2583d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getActivation(name):\n",
    "    # the hook signature\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "cityscapes_activations = []\n",
    "cityscapes_overall_iou = []\n",
    "cityscapes_iou_by_class =[]\n",
    "cityscapes_predictions = []\n",
    "\n",
    "for image,label in cityscapes_loader:\n",
    "    activation = {}\n",
    "    h = model_.middle[3].register_forward_hook(getActivation(\"bottleneck\"))\n",
    "    out = model_(image)\n",
    "    prediction = torch.argmax(out, dim=1)\n",
    "    h.remove()\n",
    "    overall_iou,iou_list = IoU_per_class(prediction,label)\n",
    "    cityscapes_overall_iou.append(overall_iou)\n",
    "    cityscapes_iou_by_class.append(iou_list)\n",
    "    prediction = prediction.detach().numpy()[0]\n",
    "    cityscapes_predictions.append(prediction)\n",
    "    instance_activation = activation[\"bottleneck\"]\n",
    "#     print(instance_activation.size())\n",
    "    # need to reshape\n",
    "    instance_activation_reshaped = instance_activation.clone().squeeze(0).reshape(256,1024).numpy()\n",
    "    cityscapes_activations.append(instance_activation_reshaped)\n",
    "#     print(instance_activation_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "503995f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cityscapes_activations = np.array(cityscapes_activations)\n",
    "cityscapes_overall_iou = np.array(cityscapes_overall_iou)\n",
    "cityscapes_iou_by_class = np.array(cityscapes_iou_by_class)\n",
    "cityscapes_predictions = np.array(cityscapes_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "94474925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 256, 256)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cityscapes_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc46fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cityscapes_predictions.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "5428323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthia_activations = []\n",
    "synthia_overall_iou = []\n",
    "synthia_iou_by_class =[]\n",
    "synthia_predictions =[]\n",
    "for image,label in synthia_loader:\n",
    "    activation = {}\n",
    "    h = model_.middle[3].register_forward_hook(getActivation(\"bottleneck\"))\n",
    "    out = model_(image)\n",
    "    prediction = torch.argmax(out, dim=1)\n",
    "    h.remove()\n",
    "    overall_iou,iou_list = IoU_per_class(prediction,label)\n",
    "    synthia_overall_iou.append(overall_iou)\n",
    "    synthia_iou_by_class.append(iou_list)\n",
    "    prediction = prediction.detach().numpy()[0]\n",
    "    synthia_predictions.append(prediction)\n",
    "    instance_activation = activation[\"bottleneck\"]\n",
    "#     print(instance_activation.size())\n",
    "    # need to reshape\n",
    "    instance_activation_reshaped = instance_activation.clone().squeeze(0).reshape(256,1024).numpy()\n",
    "    synthia_activations.append(instance_activation_reshaped)\n",
    "\n",
    "synthia_activations = np.array(synthia_activations)\n",
    "synthia_overall_iou = np.array(synthia_overall_iou)\n",
    "synthia_iou_by_class = np.array(synthia_iou_by_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "c6fc7ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_activations = np.concatenate((cityscapes_activations,synthia_activations),axis=0)\n",
    "combined_overall_iou = np.concatenate((cityscapes_overall_iou,synthia_overall_iou),axis=0)\n",
    "combined_iou_by_class = np.concatenate((cityscapes_iou_by_class,synthia_iou_by_class),axis=0)\n",
    "combined_predictions = np.concatenate((cityscapes_predictions,synthia_predictions),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "bc61d6ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 256, 256)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90456eba",
   "metadata": {},
   "source": [
    "Save predictions to paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "c430ac5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cityscapes_prediction_folder = \"D:\\zsh\\graduation\\Graduation-project-domain-shift-image-2-image\\dataset\\cityscapes_predictions\"\n",
    "cityscapes_prediction_folder_relative = os.path.relpath(cityscapes_prediction_folder, start)\n",
    "\n",
    "synthia_prediction_folder = \"D:\\zsh\\graduation\\Graduation-project-domain-shift-image-2-image\\dataset\\SYNTHIA_256\\predictions\"\n",
    "synthia_prediction_folder_relative = os.path.relpath(synthia_prediction_folder, start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "470bf770",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_paths = []\n",
    "for i, prediction in enumerate(combined_predictions):\n",
    "    name = df.iloc[i][\"name\"]\n",
    "    dataset = df.iloc[i][\"dataset\"]\n",
    "    prediction_img = class_to_color(prediction)\n",
    "    if dataset==\"Cityscapes\":\n",
    "        full_folder = cityscapes_prediction_folder\n",
    "        relative_folder = cityscapes_prediction_folder_relative\n",
    "    elif dataset==\"Synthia\":\n",
    "        full_folder = synthia_prediction_folder\n",
    "        relative_folder = synthia_prediction_folder_relative\n",
    "    \n",
    "    prediction_img = Image.fromarray(prediction_img.astype(np.uint8))\n",
    "    prediction_img.save(full_folder+\"\\\\\"+name)\n",
    "    prediction_paths.append(relative_folder+\"\\\\\"+name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e6caf7",
   "metadata": {},
   "source": [
    "* activations\n",
    "\n",
    "(next cell has 6 minute run time for 200 images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "9788b27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensionality reduction for combined instance activations\n",
    "combined_activations_2d = np.reshape(combined_activations,(200*256,1024))\n",
    "\n",
    "pca = PCA(n_components=50)\n",
    "pca_embedding = pca.fit_transform(combined_activations_2d)\n",
    "activations_tsne_embedding = TSNE(n_components=2).fit_transform(pca_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "1c859fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations_tsne_embedding_reshaped = np.reshape(activations_tsne_embedding,(200,256,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "ad72a1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('activations_embeddings.pkl', 'wb') as file:\n",
    "      \n",
    "    # A new file will be created\n",
    "    pickle.dump(activations_tsne_embedding_reshaped, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "32088776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51200, 2)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations_tsne_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "8db25d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation_df\n",
    "df_full[\"bottleneck_activations_embedding\"] = activations_tsne_embedding_reshaped.tolist()\n",
    "\n",
    "# overall_iou\n",
    "df_full[\"overall_iou\"] = combined_overall_iou\n",
    "\n",
    "# iou by class\n",
    "iou_class_df = pd.DataFrame(combined_iou_by_class,columns = [\"other_iou\",\"road_iou\",\"sidewalk_iou\",\"vegetation_iou\",\"sky_iou\",\"car_iou\"])\n",
    "df_full = pd.concat((df_full,iou_class_df),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "03dbd7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full[\"prediction_path\"] = prediction_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "c8637485",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rearrange = df_full[['name','dataset', 'image_path', 'label_path','prediction_path', 'similar_image_paths','similar_IoU_score', \n",
    "                       'other_ratio', 'road_ratio', 'sidewalk_ratio','vegetation_ratio', 'sky_ratio', 'car_ratio', \n",
    "                       'simple_tsne_1','simple_tsne_2', 'meaningful_tsne_1', 'meaningful_tsne_2',\n",
    "       'bottleneck_activations_embedding', 'overall_iou', 'other_iou',\n",
    "       'road_iou', 'sidewalk_iou', 'vegetation_iou', 'sky_iou', 'car_iou']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "7cc4ca25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['name', 'image_path', 'dataset', 'label_path', 'similar_image_paths',\n",
       "       'similar_IoU_score', 'other_ratio', 'road_ratio', 'sidewalk_ratio',\n",
       "       'vegetation_ratio', 'sky_ratio', 'car_ratio', 'simple_tsne_1',\n",
       "       'simple_tsne_2', 'meaningful_tsne_1', 'meaningful_tsne_2',\n",
       "       'bottleneck_activations_embedding', 'overall_iou', 'other_iou',\n",
       "       'road_iou', 'sidewalk_iou', 'vegetation_iou', 'sky_iou', 'car_iou',\n",
       "       'prediction_path'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "aedc761a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>dataset</th>\n",
       "      <th>image_path</th>\n",
       "      <th>label_path</th>\n",
       "      <th>prediction_path</th>\n",
       "      <th>similar_image_paths</th>\n",
       "      <th>similar_IoU_score</th>\n",
       "      <th>other_ratio</th>\n",
       "      <th>road_ratio</th>\n",
       "      <th>sidewalk_ratio</th>\n",
       "      <th>...</th>\n",
       "      <th>meaningful_tsne_1</th>\n",
       "      <th>meaningful_tsne_2</th>\n",
       "      <th>bottleneck_activations_embedding</th>\n",
       "      <th>overall_iou</th>\n",
       "      <th>other_iou</th>\n",
       "      <th>road_iou</th>\n",
       "      <th>sidewalk_iou</th>\n",
       "      <th>vegetation_iou</th>\n",
       "      <th>sky_iou</th>\n",
       "      <th>car_iou</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.jpeg</td>\n",
       "      <td>Cityscapes</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\14.jpeg</td>\n",
       "      <td>dataset\\cityscapes_labels_sample\\14.jpeg</td>\n",
       "      <td>dataset\\cityscapes_predictions\\14.jpeg</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0002688.png</td>\n",
       "      <td>0.475754</td>\n",
       "      <td>0.268173</td>\n",
       "      <td>0.380219</td>\n",
       "      <td>0.054855</td>\n",
       "      <td>...</td>\n",
       "      <td>6.933020</td>\n",
       "      <td>7.210295</td>\n",
       "      <td>[[9.249624252319336, 25.688404083251953], [-24...</td>\n",
       "      <td>0.880295</td>\n",
       "      <td>0.719708</td>\n",
       "      <td>0.944721</td>\n",
       "      <td>0.695616</td>\n",
       "      <td>0.718850</td>\n",
       "      <td>0.862466</td>\n",
       "      <td>0.311558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87.jpeg</td>\n",
       "      <td>Cityscapes</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\87.jpeg</td>\n",
       "      <td>dataset\\cityscapes_labels_sample\\87.jpeg</td>\n",
       "      <td>dataset\\cityscapes_predictions\\87.jpeg</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0002688.png</td>\n",
       "      <td>0.480301</td>\n",
       "      <td>0.390427</td>\n",
       "      <td>0.322311</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.218904</td>\n",
       "      <td>12.409754</td>\n",
       "      <td>[[9.378799438476562, 25.773765563964844], [-23...</td>\n",
       "      <td>0.905487</td>\n",
       "      <td>0.788071</td>\n",
       "      <td>0.962711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.739955</td>\n",
       "      <td>0.164607</td>\n",
       "      <td>0.863164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>189.jpeg</td>\n",
       "      <td>Cityscapes</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\189.jpeg</td>\n",
       "      <td>dataset\\cityscapes_labels_sample\\189.jpeg</td>\n",
       "      <td>dataset\\cityscapes_predictions\\189.jpeg</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0004607.png</td>\n",
       "      <td>0.653656</td>\n",
       "      <td>0.615875</td>\n",
       "      <td>0.264603</td>\n",
       "      <td>0.010590</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.544376</td>\n",
       "      <td>10.230064</td>\n",
       "      <td>[[9.018597602844238, 25.574705123901367], [-24...</td>\n",
       "      <td>0.849472</td>\n",
       "      <td>0.780169</td>\n",
       "      <td>0.925425</td>\n",
       "      <td>0.127430</td>\n",
       "      <td>0.278427</td>\n",
       "      <td>0.892169</td>\n",
       "      <td>0.811478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>167.jpeg</td>\n",
       "      <td>Cityscapes</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\167.jpeg</td>\n",
       "      <td>dataset\\cityscapes_labels_sample\\167.jpeg</td>\n",
       "      <td>dataset\\cityscapes_predictions\\167.jpeg</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0005291.png</td>\n",
       "      <td>0.453262</td>\n",
       "      <td>0.235931</td>\n",
       "      <td>0.296295</td>\n",
       "      <td>0.021698</td>\n",
       "      <td>...</td>\n",
       "      <td>3.992030</td>\n",
       "      <td>5.020967</td>\n",
       "      <td>[[10.460427284240723, 27.210540771484375], [-2...</td>\n",
       "      <td>0.691772</td>\n",
       "      <td>0.462270</td>\n",
       "      <td>0.940231</td>\n",
       "      <td>0.192641</td>\n",
       "      <td>0.402005</td>\n",
       "      <td>0.920625</td>\n",
       "      <td>0.008711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>439.jpeg</td>\n",
       "      <td>Cityscapes</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\439.jpeg</td>\n",
       "      <td>dataset\\cityscapes_labels_sample\\439.jpeg</td>\n",
       "      <td>dataset\\cityscapes_predictions\\439.jpeg</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0002688.png</td>\n",
       "      <td>0.522873</td>\n",
       "      <td>0.501968</td>\n",
       "      <td>0.312622</td>\n",
       "      <td>0.026047</td>\n",
       "      <td>...</td>\n",
       "      <td>8.317305</td>\n",
       "      <td>5.757763</td>\n",
       "      <td>[[10.263081550598145, 25.39412498474121], [-23...</td>\n",
       "      <td>0.738602</td>\n",
       "      <td>0.669778</td>\n",
       "      <td>0.695943</td>\n",
       "      <td>0.133251</td>\n",
       "      <td>0.536060</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0009204.png</td>\n",
       "      <td>Synthia</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0009204.png</td>\n",
       "      <td>dataset\\SYNTHIA_256\\new_labels\\0009204.png</td>\n",
       "      <td>dataset\\SYNTHIA_256\\predictions\\0009204.png</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\188.jpeg</td>\n",
       "      <td>0.492752</td>\n",
       "      <td>0.414795</td>\n",
       "      <td>0.169708</td>\n",
       "      <td>0.330994</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063736</td>\n",
       "      <td>-16.626930</td>\n",
       "      <td>[[12.014287948608398, 21.802379608154297], [-2...</td>\n",
       "      <td>0.303146</td>\n",
       "      <td>0.290828</td>\n",
       "      <td>0.224877</td>\n",
       "      <td>0.024413</td>\n",
       "      <td>0.088516</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0000063.png</td>\n",
       "      <td>Synthia</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0000063.png</td>\n",
       "      <td>dataset\\SYNTHIA_256\\new_labels\\0000063.png</td>\n",
       "      <td>dataset\\SYNTHIA_256\\predictions\\0000063.png</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\174.jpeg</td>\n",
       "      <td>0.481476</td>\n",
       "      <td>0.356415</td>\n",
       "      <td>0.243332</td>\n",
       "      <td>0.053818</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.205073</td>\n",
       "      <td>-8.018494</td>\n",
       "      <td>[[12.287400245666504, 25.470184326171875], [-2...</td>\n",
       "      <td>0.563889</td>\n",
       "      <td>0.383060</td>\n",
       "      <td>0.525158</td>\n",
       "      <td>0.087156</td>\n",
       "      <td>0.476979</td>\n",
       "      <td>0.426459</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0000582.png</td>\n",
       "      <td>Synthia</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0000582.png</td>\n",
       "      <td>dataset\\SYNTHIA_256\\new_labels\\0000582.png</td>\n",
       "      <td>dataset\\SYNTHIA_256\\predictions\\0000582.png</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\248.jpeg</td>\n",
       "      <td>0.512939</td>\n",
       "      <td>0.256805</td>\n",
       "      <td>0.283630</td>\n",
       "      <td>0.038498</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.830279</td>\n",
       "      <td>-8.390073</td>\n",
       "      <td>[[13.566570281982422, 24.40737533569336], [-22...</td>\n",
       "      <td>0.565704</td>\n",
       "      <td>0.393866</td>\n",
       "      <td>0.488740</td>\n",
       "      <td>0.142928</td>\n",
       "      <td>0.387739</td>\n",
       "      <td>0.503347</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0005816.png</td>\n",
       "      <td>Synthia</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0005816.png</td>\n",
       "      <td>dataset\\SYNTHIA_256\\new_labels\\0005816.png</td>\n",
       "      <td>dataset\\SYNTHIA_256\\predictions\\0005816.png</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\147.jpeg</td>\n",
       "      <td>0.430588</td>\n",
       "      <td>0.386841</td>\n",
       "      <td>0.029251</td>\n",
       "      <td>0.383972</td>\n",
       "      <td>...</td>\n",
       "      <td>0.840205</td>\n",
       "      <td>-6.707541</td>\n",
       "      <td>[[10.129311561584473, 24.113561630249023], [-2...</td>\n",
       "      <td>0.364288</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.016676</td>\n",
       "      <td>0.106556</td>\n",
       "      <td>0.315102</td>\n",
       "      <td>0.467021</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0003277.png</td>\n",
       "      <td>Synthia</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0003277.png</td>\n",
       "      <td>dataset\\SYNTHIA_256\\new_labels\\0003277.png</td>\n",
       "      <td>dataset\\SYNTHIA_256\\predictions\\0003277.png</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\222.jpeg</td>\n",
       "      <td>0.584671</td>\n",
       "      <td>0.414612</td>\n",
       "      <td>0.332169</td>\n",
       "      <td>0.162247</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.329117</td>\n",
       "      <td>-13.216610</td>\n",
       "      <td>[[12.378683090209961, 23.570796966552734], [-2...</td>\n",
       "      <td>0.575821</td>\n",
       "      <td>0.549523</td>\n",
       "      <td>0.453318</td>\n",
       "      <td>0.220380</td>\n",
       "      <td>0.132010</td>\n",
       "      <td>0.398249</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            name     dataset                                   image_path  \\\n",
       "0        14.jpeg  Cityscapes   dataset\\original_cityscapes_inputs\\14.jpeg   \n",
       "1        87.jpeg  Cityscapes   dataset\\original_cityscapes_inputs\\87.jpeg   \n",
       "2       189.jpeg  Cityscapes  dataset\\original_cityscapes_inputs\\189.jpeg   \n",
       "3       167.jpeg  Cityscapes  dataset\\original_cityscapes_inputs\\167.jpeg   \n",
       "4       439.jpeg  Cityscapes  dataset\\original_cityscapes_inputs\\439.jpeg   \n",
       "..           ...         ...                                          ...   \n",
       "195  0009204.png     Synthia        dataset\\SYNTHIA_256\\image\\0009204.png   \n",
       "196  0000063.png     Synthia        dataset\\SYNTHIA_256\\image\\0000063.png   \n",
       "197  0000582.png     Synthia        dataset\\SYNTHIA_256\\image\\0000582.png   \n",
       "198  0005816.png     Synthia        dataset\\SYNTHIA_256\\image\\0005816.png   \n",
       "199  0003277.png     Synthia        dataset\\SYNTHIA_256\\image\\0003277.png   \n",
       "\n",
       "                                     label_path  \\\n",
       "0      dataset\\cityscapes_labels_sample\\14.jpeg   \n",
       "1      dataset\\cityscapes_labels_sample\\87.jpeg   \n",
       "2     dataset\\cityscapes_labels_sample\\189.jpeg   \n",
       "3     dataset\\cityscapes_labels_sample\\167.jpeg   \n",
       "4     dataset\\cityscapes_labels_sample\\439.jpeg   \n",
       "..                                          ...   \n",
       "195  dataset\\SYNTHIA_256\\new_labels\\0009204.png   \n",
       "196  dataset\\SYNTHIA_256\\new_labels\\0000063.png   \n",
       "197  dataset\\SYNTHIA_256\\new_labels\\0000582.png   \n",
       "198  dataset\\SYNTHIA_256\\new_labels\\0005816.png   \n",
       "199  dataset\\SYNTHIA_256\\new_labels\\0003277.png   \n",
       "\n",
       "                                 prediction_path  \\\n",
       "0         dataset\\cityscapes_predictions\\14.jpeg   \n",
       "1         dataset\\cityscapes_predictions\\87.jpeg   \n",
       "2        dataset\\cityscapes_predictions\\189.jpeg   \n",
       "3        dataset\\cityscapes_predictions\\167.jpeg   \n",
       "4        dataset\\cityscapes_predictions\\439.jpeg   \n",
       "..                                           ...   \n",
       "195  dataset\\SYNTHIA_256\\predictions\\0009204.png   \n",
       "196  dataset\\SYNTHIA_256\\predictions\\0000063.png   \n",
       "197  dataset\\SYNTHIA_256\\predictions\\0000582.png   \n",
       "198  dataset\\SYNTHIA_256\\predictions\\0005816.png   \n",
       "199  dataset\\SYNTHIA_256\\predictions\\0003277.png   \n",
       "\n",
       "                             similar_image_paths  similar_IoU_score  \\\n",
       "0          dataset\\SYNTHIA_256\\image\\0002688.png           0.475754   \n",
       "1          dataset\\SYNTHIA_256\\image\\0002688.png           0.480301   \n",
       "2          dataset\\SYNTHIA_256\\image\\0004607.png           0.653656   \n",
       "3          dataset\\SYNTHIA_256\\image\\0005291.png           0.453262   \n",
       "4          dataset\\SYNTHIA_256\\image\\0002688.png           0.522873   \n",
       "..                                           ...                ...   \n",
       "195  dataset\\original_cityscapes_inputs\\188.jpeg           0.492752   \n",
       "196  dataset\\original_cityscapes_inputs\\174.jpeg           0.481476   \n",
       "197  dataset\\original_cityscapes_inputs\\248.jpeg           0.512939   \n",
       "198  dataset\\original_cityscapes_inputs\\147.jpeg           0.430588   \n",
       "199  dataset\\original_cityscapes_inputs\\222.jpeg           0.584671   \n",
       "\n",
       "     other_ratio  road_ratio  sidewalk_ratio  ...  meaningful_tsne_1  \\\n",
       "0       0.268173    0.380219        0.054855  ...           6.933020   \n",
       "1       0.390427    0.322311        0.000000  ...           4.218904   \n",
       "2       0.615875    0.264603        0.010590  ...          -1.544376   \n",
       "3       0.235931    0.296295        0.021698  ...           3.992030   \n",
       "4       0.501968    0.312622        0.026047  ...           8.317305   \n",
       "..           ...         ...             ...  ...                ...   \n",
       "195     0.414795    0.169708        0.330994  ...          -0.063736   \n",
       "196     0.356415    0.243332        0.053818  ...          -9.205073   \n",
       "197     0.256805    0.283630        0.038498  ...         -10.830279   \n",
       "198     0.386841    0.029251        0.383972  ...           0.840205   \n",
       "199     0.414612    0.332169        0.162247  ...          -3.329117   \n",
       "\n",
       "     meaningful_tsne_2                   bottleneck_activations_embedding  \\\n",
       "0             7.210295  [[9.249624252319336, 25.688404083251953], [-24...   \n",
       "1            12.409754  [[9.378799438476562, 25.773765563964844], [-23...   \n",
       "2            10.230064  [[9.018597602844238, 25.574705123901367], [-24...   \n",
       "3             5.020967  [[10.460427284240723, 27.210540771484375], [-2...   \n",
       "4             5.757763  [[10.263081550598145, 25.39412498474121], [-23...   \n",
       "..                 ...                                                ...   \n",
       "195         -16.626930  [[12.014287948608398, 21.802379608154297], [-2...   \n",
       "196          -8.018494  [[12.287400245666504, 25.470184326171875], [-2...   \n",
       "197          -8.390073  [[13.566570281982422, 24.40737533569336], [-22...   \n",
       "198          -6.707541  [[10.129311561584473, 24.113561630249023], [-2...   \n",
       "199         -13.216610  [[12.378683090209961, 23.570796966552734], [-2...   \n",
       "\n",
       "     overall_iou  other_iou  road_iou  sidewalk_iou vegetation_iou   sky_iou  \\\n",
       "0       0.880295   0.719708  0.944721      0.695616       0.718850  0.862466   \n",
       "1       0.905487   0.788071  0.962711      0.000000       0.739955  0.164607   \n",
       "2       0.849472   0.780169  0.925425      0.127430       0.278427  0.892169   \n",
       "3       0.691772   0.462270  0.940231      0.192641       0.402005  0.920625   \n",
       "4       0.738602   0.669778  0.695943      0.133251       0.536060  0.000000   \n",
       "..           ...        ...       ...           ...            ...       ...   \n",
       "195     0.303146   0.290828  0.224877      0.024413       0.088516  0.000000   \n",
       "196     0.563889   0.383060  0.525158      0.087156       0.476979  0.426459   \n",
       "197     0.565704   0.393866  0.488740      0.142928       0.387739  0.503347   \n",
       "198     0.364288   0.360000  0.016676      0.106556       0.315102  0.467021   \n",
       "199     0.575821   0.549523  0.453318      0.220380       0.132010  0.398249   \n",
       "\n",
       "      car_iou  \n",
       "0    0.311558  \n",
       "1    0.863164  \n",
       "2    0.811478  \n",
       "3    0.008711  \n",
       "4    0.033378  \n",
       "..        ...  \n",
       "195  0.000000  \n",
       "196  0.000000  \n",
       "197  0.000000  \n",
       "198  0.000000  \n",
       "199  0.000000  \n",
       "\n",
       "[200 rows x 25 columns]"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "4ae3e25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rearrange.to_csv(\"system_df_full.csv\",index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c81f254",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
