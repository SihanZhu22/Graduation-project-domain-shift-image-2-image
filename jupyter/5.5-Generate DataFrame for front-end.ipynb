{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14279bdc",
   "metadata": {},
   "source": [
    "This file is based on \"4.27- Link input with activations.ipynb\", with updated data structure (different list structures -> list of tuples). There are also more data that are useful for making visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2256c39f",
   "metadata": {},
   "source": [
    "1. Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3604ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "\n",
    "# Use IoU instead\n",
    "# import re\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb04c5a",
   "metadata": {},
   "source": [
    "2. Decide the procedure by grouping the different columns together"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1736c4ba",
   "metadata": {},
   "source": [
    "For cityscapes and synthia separately (and then concatenate):\n",
    "\n",
    "* name, image_path, dataset\n",
    "\n",
    "* label_path:\n",
    "    * first get labels (in class format)\n",
    "        * for synthia, need to transform to class format specifically\n",
    "            * for calculating similarity\n",
    "        * also from class to color as well, because use the color of cityscapes\n",
    "            * for saving the label and getting the label path\n",
    "        * get the saved path and save it in a list\n",
    "    \n",
    "* similar_image_paths，similar_IoU_score (Note: called IoU_score originally): finding the most similar masks from another dataset\n",
    "* Generate from os list of images:\n",
    "    * class distribution\n",
    "        * (other_ratio, road_ratio, sidewalk_ratio, vegetation_ratio, sky_ratio, car_ratio)\n",
    "    * embedding of input (originally: tsne_1, tsne_2)\n",
    "        simple dimensionality reduction (simple_tsne_1,simple_tsne_2)\n",
    "        and also the embedding from classification model(meaningful_tsne_1,meaningful_tsne_1)\n",
    "* bottleneck_activations_embedding, prediction_path, and performance\n",
    "    * save output locally, and then add path (prediction_path)\n",
    "    * performance: (other_IoU, road_IoU, sidewalk_IoU, vegetation_IoU, sky_IoU, car_IoU)\n",
    "        **How do you get the IoU per class (check original paper)**\n",
    "\n",
    "**Make sure that the column names are the same as current！！！**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4459a8b",
   "metadata": {},
   "source": [
    "# name, image_path, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68c45a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path of the folders\n",
    "image_path_cityscapes = \"D:\\zsh\\graduation\\Graduation-project-domain-shift-image-2-image\\dataset\\original_cityscapes_inputs\"\n",
    "image_path_synthia = \"D:\\zsh\\graduation\\Graduation-project-domain-shift-image-2-image\\dataset\\SYNTHIA_256\\image\"\n",
    "\n",
    "start = \"D:\\zsh\\graduation\\Graduation-project-domain-shift-image-2-image\"\n",
    "relative_img_path_cityscapes = os.path.relpath(image_path_cityscapes, start)\n",
    "relative_img_path_synthia = os.path.relpath(image_path_synthia, start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67f72734",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_number = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "884c3fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cityscapes_names= os.listdir(image_path_cityscapes)\n",
    "random.seed(55)\n",
    "cityscapes_names_sample = random.sample(cityscapes_names,sample_number)\n",
    "\n",
    "synthia_names = os.listdir(image_path_synthia)\n",
    "# randomly select 100 images to load in to numpy array\n",
    "random.seed(55)\n",
    "synthia_names_sample = random.sample(synthia_names,sample_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0a96b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cityscapes\n",
    "cityscapes_initial_info = []\n",
    "\n",
    "for name in cityscapes_names_sample:\n",
    "    image = Image.open(image_path_cityscapes+\"\\\\\"+name).convert(\"RGB\")\n",
    "    image_path = relative_img_path_cityscapes+\"\\\\\"+name\n",
    "    cityscapes_initial_info.append(tuple((name,image_path,\"Cityscapes\")))\n",
    "\n",
    "cityscapes_initial_info = np.array(cityscapes_initial_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ff0eff",
   "metadata": {},
   "source": [
    "# Worked until here:\n",
    "\n",
    "problem: it's still not list of tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "11cf1a79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cityscapes_initial_info[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f874ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthia\n",
    "\n",
    "synthia_initial_info = []\n",
    "\n",
    "for name in synthia_names_100:\n",
    "#     print(name)\n",
    "#     print(resize_image_path_synthia)\n",
    "#     print(resize_image_path_synthia+\"\\\\\"+name)\n",
    "    image = Image.open(resize_image_path_synthia+\"\\\\\"+name).convert(\"RGB\")\n",
    "    \n",
    "    synthia_100.append(np.array(image))\n",
    "    synthia_image_path_100.append(relative_path_synthia+\"\\\\\"+name)\n",
    "    synthia_label_path_100.append(relative_path_synthia_labels+\"\\\\\"+name)\n",
    "    label = Image.open(resize_label_path_synthia+\"\\\\\"+name).convert(\"RGB\")\n",
    "    synthia_label_100_initial.append(np.array(label))\n",
    "\n",
    "synthia_100 = np.array(synthia_100)\n",
    "synthia_image_path_100 = np.array(synthia_image_path_100)\n",
    "synthia_label_path_100= np.array(synthia_label_path_100)\n",
    "synthia_label_100_initial = np.array(synthia_label_100_initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c844a6",
   "metadata": {},
   "source": [
    "# Synthia color transformation code (for later use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc3c71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthia_colors = [[  0,   0,   0], # void\n",
    "         [70,130, 180], # sky\n",
    "         [70,70,70], # building\n",
    "        [128, 64, 128], # road\n",
    "        [244, 35, 232], # sidewalk\n",
    "         [64,64,128], # fense\n",
    "         [107,142,35], # vegetation\t\n",
    "        [153, 153, 153], # pole\n",
    "        [0, 0, 142], # car\n",
    "        [220, 220, 0],  # traffic sign\n",
    "        [220, 20, 60], # pedestrian\n",
    "        [119, 11, 32], # bicycle\n",
    "        [0, 0, 230], # motorcycle\n",
    "        [250,170,160], # parking-slot\n",
    "        [128,64,64], # road-work\n",
    "        [250,170,30], # traffic light\n",
    "        [152, 251, 152], # terrain\n",
    "        [255, 0, 0], # rider\n",
    "        [0, 0, 70], # truck\n",
    "        [0, 60, 100], # bus\n",
    "        [0, 80, 100], # train\n",
    "        [102, 102, 156]# wall, lanemarking\n",
    "    ]\n",
    "\n",
    "\n",
    "category_map = {\n",
    "    0: 0,\n",
    "    1: 4,\n",
    "    2: 0,\n",
    "    3: 1,\n",
    "    4: 2,\n",
    "    5: 0,\n",
    "    6: 3,\n",
    "    7: 0,\n",
    "    8: 5,\n",
    "    9: 0,\n",
    "    10: 0,\n",
    "    11: 0,\n",
    "    12: 0,\n",
    "    13: 0,\n",
    "    14: 0,\n",
    "    15: 0,\n",
    "    16: 0,\n",
    "    17: 0,\n",
    "    18: 0,\n",
    "    19: 0,\n",
    "    20: 0,\n",
    "    21: 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa629548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# color to class\n",
    "def color_to_class(label):\n",
    "    # create new empty mask\n",
    "    mask = np.zeros(shape=(label.shape[0], label.shape[1]), dtype = np.int32)\n",
    "    # iterate through two dimensions\n",
    "    for row in range(label.shape[0]):\n",
    "        for col in range(label.shape[1]):\n",
    "            a = label[row, col,:]\n",
    "            # distance between this pixel and the original pixel\n",
    "            d = cdist(np.array([a]),np.array(synthia_colors))\n",
    "            idx = np.argmin(d)\n",
    "            new_idx = category_map[idx]\n",
    "            mask[row, col] = new_idx\n",
    "    mask = np.reshape(mask, (mask.shape[0], mask.shape[1]))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9907a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class to color: useful for displaying the similar images later\n",
    "colors = [[  0,   0,   0],\n",
    "          [128, 64, 128],# road\n",
    "          [244, 35, 232], # sidewalk\n",
    "          [107, 142, 35],# vegetation\n",
    "          [70, 130, 180], # sky\n",
    "          [0, 0, 142], # car\n",
    "         ]\n",
    "\n",
    "def class_to_color(labels):\n",
    "    label_colors = np.zeros((256,256,3))\n",
    "    \n",
    "    for i,row in enumerate(labels):\n",
    "        for j,pixel in enumerate(row):\n",
    "            label_colors[i,j] = colors[pixel]\n",
    "    \n",
    "    return label_colors.astype(int)  # make each pixel value an integer to visualize it better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c35f82",
   "metadata": {},
   "source": [
    "# name, image path, label path, labels (in class format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453f3662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_masks_synthia(labels):\n",
    "    masks = []\n",
    "    for label in labels:\n",
    "        mask = color_to_class(label)\n",
    "        masks.append(mask)\n",
    "    masks = np.array(masks)\n",
    "\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c5a7d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
