{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bebe42c",
   "metadata": {},
   "source": [
    "This file is based on \"5.5-Generate DataFrame for front-end.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67141c6f",
   "metadata": {},
   "source": [
    "1. Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4430d195",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\zsh\\graduation\\grad_env_take_2\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import umap\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Use IoU instead\n",
    "# import re\n",
    "from scipy.spatial.distance import cdist\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7342489a",
   "metadata": {},
   "source": [
    "###### 2. Decide the procedure by grouping the different columns together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e738012",
   "metadata": {},
   "source": [
    "For cityscapes and synthia separately (and then concatenate):\n",
    "\n",
    "* name, image_path, dataset\n",
    "\n",
    "* label_path:\n",
    "    * first get labels (in class format)\n",
    "        * for synthia, need to transform to class format specifically\n",
    "            * for calculating similarity\n",
    "        * also from class to color as well, because use the color of cityscapes\n",
    "            * for saving the label and getting the label path\n",
    "        * get the saved path and save it in a list\n",
    "    \n",
    "* similar_image_paths，similar_IoU_score (Note: called IoU_score originally): finding the most similar masks from another dataset\n",
    "* Generate from os list of images:\n",
    "    * class distribution\n",
    "        * (other_ratio, road_ratio, sidewalk_ratio, vegetation_ratio, sky_ratio, car_ratio)\n",
    "    * embedding of input (originally: tsne_1, tsne_2)\n",
    "        simple dimensionality reduction (simple_tsne_1,simple_tsne_2)\n",
    "        and also the embedding from classification model(meaningful_tsne_1,meaningful_tsne_1)\n",
    "* bottleneck_activations_embedding, prediction_path, and performance\n",
    "    * save output locally, and then add path (prediction_path)\n",
    "    * performance: (other_IoU, road_IoU, sidewalk_IoU, vegetation_IoU, sky_IoU, car_IoU)\n",
    "\n",
    "**Make sure that the column names are the same as current！！！**\n",
    "\n",
    "**Remember to also save id**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c096fb",
   "metadata": {},
   "source": [
    "# name, image_path, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7517ead8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path of the image folders for the model\n",
    "image_path_cityscapes = \"D:\\zsh\\graduation\\Graduation-project-domain-shift-image-2-image\\dataset\\original_cityscapes_inputs\" # 500 images in total\n",
    "image_path_synthia = \"D:\\zsh\\graduation\\Graduation-project-domain-shift-image-2-image\\dataset\\SYNTHIA_256\\image\" # 9400 images in total\n",
    "\n",
    "# path of image folders for displaying in the system\n",
    "\n",
    "start = \"D:\\zsh\\graduation\\Graduation-project-domain-shift-image-2-image\"\n",
    "relative_img_path_cityscapes = os.path.relpath(image_path_cityscapes, start)\n",
    "relative_img_path_synthia = os.path.relpath(image_path_synthia, start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8455c664",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_number = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bd73b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cityscapes_names= os.listdir(image_path_cityscapes)\n",
    "random.seed(55)\n",
    "cityscapes_names_sample = random.sample(cityscapes_names,sample_number)\n",
    "\n",
    "synthia_names = os.listdir(image_path_synthia)\n",
    "# randomly select 100 images to load in to numpy array\n",
    "random.seed(55)\n",
    "synthia_names_sample = random.sample(synthia_names,sample_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71168c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cityscapes\n",
    "cityscapes_initial_info = [] # include name, image path and dataset name\n",
    "cityscapes_img_sample = []\n",
    "\n",
    "for name in cityscapes_names_sample:\n",
    "    image = Image.open(image_path_cityscapes+\"\\\\\"+name).convert(\"RGB\")\n",
    "    image_path = relative_img_path_cityscapes+\"\\\\\"+name\n",
    "    \n",
    "    cityscapes_img_sample.append(np.array(image))\n",
    "    cityscapes_initial_info.append((name,image_path,\"Cityscapes\"))\n",
    "\n",
    "cityscapes_initial_info = np.array(cityscapes_initial_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d84629f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['14.jpeg', 'dataset\\\\original_cityscapes_inputs\\\\14.jpeg',\n",
       "        'Cityscapes'],\n",
       "       ['87.jpeg', 'dataset\\\\original_cityscapes_inputs\\\\87.jpeg',\n",
       "        'Cityscapes'],\n",
       "       ['189.jpeg', 'dataset\\\\original_cityscapes_inputs\\\\189.jpeg',\n",
       "        'Cityscapes'],\n",
       "       ['167.jpeg', 'dataset\\\\original_cityscapes_inputs\\\\167.jpeg',\n",
       "        'Cityscapes'],\n",
       "       ['439.jpeg', 'dataset\\\\original_cityscapes_inputs\\\\439.jpeg',\n",
       "        'Cityscapes']], dtype='<U43')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cityscapes_initial_info[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "756d2269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthia\n",
    "synthia_initial_info = [] # include name, image path and dataset name\n",
    "synthia_img_sample = []\n",
    "\n",
    "for name in synthia_names_sample:\n",
    "    image = Image.open(image_path_synthia+\"\\\\\"+name).convert(\"RGB\")\n",
    "    image_path = relative_img_path_synthia+\"\\\\\"+name\n",
    "    synthia_initial_info.append((name, image_path,\"Synthia\"))\n",
    "    synthia_img_sample.append(np.array(image))\n",
    "\n",
    "synthia_initial_info = np.array(synthia_initial_info)\n",
    "synthia_img_sample = np.array(synthia_img_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d69e826b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(synthia_initial_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd7ec0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_initial_info = np.concatenate((cityscapes_initial_info,synthia_initial_info),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cc0fef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(combined_initial_info,columns = [\"name\", \"image_path\",\"dataset\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d709119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>image_path</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.jpeg</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\14.jpeg</td>\n",
       "      <td>Cityscapes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87.jpeg</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\87.jpeg</td>\n",
       "      <td>Cityscapes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>189.jpeg</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\189.jpeg</td>\n",
       "      <td>Cityscapes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>167.jpeg</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\167.jpeg</td>\n",
       "      <td>Cityscapes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>439.jpeg</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\439.jpeg</td>\n",
       "      <td>Cityscapes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       name                                   image_path     dataset\n",
       "0   14.jpeg   dataset\\original_cityscapes_inputs\\14.jpeg  Cityscapes\n",
       "1   87.jpeg   dataset\\original_cityscapes_inputs\\87.jpeg  Cityscapes\n",
       "2  189.jpeg  dataset\\original_cityscapes_inputs\\189.jpeg  Cityscapes\n",
       "3  167.jpeg  dataset\\original_cityscapes_inputs\\167.jpeg  Cityscapes\n",
       "4  439.jpeg  dataset\\original_cityscapes_inputs\\439.jpeg  Cityscapes"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1c5c61",
   "metadata": {},
   "source": [
    "# Synthia color transformation code (for later use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6996559",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthia_colors = [[  0,   0,   0], # void\n",
    "         [70,130, 180], # sky\n",
    "         [70,70,70], # building\n",
    "        [128, 64, 128], # road\n",
    "        [244, 35, 232], # sidewalk\n",
    "         [64,64,128], # fense\n",
    "         [107,142,35], # vegetation\t\n",
    "        [153, 153, 153], # pole\n",
    "        [0, 0, 142], # car\n",
    "        [220, 220, 0],  # traffic sign\n",
    "        [220, 20, 60], # pedestrian\n",
    "        [119, 11, 32], # bicycle\n",
    "        [0, 0, 230], # motorcycle\n",
    "        [250,170,160], # parking-slot\n",
    "        [128,64,64], # road-work\n",
    "        [250,170,30], # traffic light\n",
    "        [152, 251, 152], # terrain\n",
    "        [255, 0, 0], # rider\n",
    "        [0, 0, 70], # truck\n",
    "        [0, 60, 100], # bus\n",
    "        [0, 80, 100], # train\n",
    "        [102, 102, 156]# wall, lanemarking\n",
    "    ]\n",
    "\n",
    "\n",
    "category_map = {\n",
    "    0: 0,\n",
    "    1: 4,\n",
    "    2: 0,\n",
    "    3: 1,\n",
    "    4: 2,\n",
    "    5: 0,\n",
    "    6: 3,\n",
    "    7: 0,\n",
    "    8: 5,\n",
    "    9: 0,\n",
    "    10: 0,\n",
    "    11: 0,\n",
    "    12: 0,\n",
    "    13: 0,\n",
    "    14: 0,\n",
    "    15: 0,\n",
    "    16: 0,\n",
    "    17: 0,\n",
    "    18: 0,\n",
    "    19: 0,\n",
    "    20: 0,\n",
    "    21: 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79ebb591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# color to class\n",
    "def color_to_class(label):\n",
    "    # create new empty mask\n",
    "    mask = np.zeros(shape=(label.shape[0], label.shape[1]), dtype = np.int32)\n",
    "    # iterate through two dimensions\n",
    "    for row in range(label.shape[0]):\n",
    "        for col in range(label.shape[1]):\n",
    "            a = label[row, col,:]\n",
    "            # distance between this pixel and the original pixel\n",
    "            d = cdist(np.array([a]),np.array(synthia_colors))\n",
    "            idx = np.argmin(d)\n",
    "            new_idx = category_map[idx]\n",
    "            mask[row, col] = new_idx\n",
    "    mask = np.reshape(mask, (mask.shape[0], mask.shape[1]))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b652eeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the color for classes\n",
    "colors = [[  0,   0,   0],\n",
    "          [128, 64, 128],# road\n",
    "          [244, 35, 232], # sidewalk\n",
    "          [107, 142, 35],# vegetation\n",
    "          [70, 130, 180], # sky\n",
    "          [0, 0, 142], # car\n",
    "         ]\n",
    "\n",
    "# class to color: useful for displaying the similar images later\n",
    "def class_to_color(labels):\n",
    "    label_colors = np.zeros((256,256,3))\n",
    "    \n",
    "    for i,row in enumerate(labels):\n",
    "        for j,pixel in enumerate(row):\n",
    "            label_colors[i,j] = colors[pixel]\n",
    "    \n",
    "    return label_colors.astype(int)  # make each pixel value an integer to visualize it better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88262b2",
   "metadata": {},
   "source": [
    "# label_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65de755",
   "metadata": {},
   "source": [
    "Cityscapes labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4553aa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relative paths for cityscapes\n",
    "cityscpaes_save_labels_path = \"D:\\zsh\\graduation\\Graduation-project-domain-shift-image-2-image\\dataset\\cityscapes_labels_sample\"\n",
    "cityscapes_label_folder_relative = os.path.relpath(cityscpaes_save_labels_path, start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76485fd1",
   "metadata": {},
   "source": [
    "**For now: still use the sample of labels. Possibly changing to something else later on**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "672ccc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels of cityscapes\n",
    "pickle_file = os.path.join(\"D:\\zsh\\graduation\\Graduation-project-domain-shift-image-2-image\\dataset\\\\5_classes_preprocessed\", \"validation_label_classes.pkl\")\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    Y_test = pickle.load(f)\n",
    "    \n",
    "cityscapes_labels = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b47d538",
   "metadata": {},
   "source": [
    "Generate and save labels for the subset for displaying in the VA system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6e57a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "cityscapes_labels_sample= []\n",
    "cityscapes_label_path_sample = []\n",
    "\n",
    "for name in cityscapes_names_sample:\n",
    "    ind = int(name.split('.')[0]) # get the cityscapes index based on names\n",
    "    label = cityscapes_labels[ind] # get corresponding labels based on indices\n",
    "    label_path = cityscapes_label_folder_relative+\"\\\\\"+name\n",
    "    \n",
    "    cityscapes_labels_sample.append(label)\n",
    "    cityscapes_label_path_sample.append(label_path)\n",
    "\n",
    "cityscapes_labels_sample = np.array(cityscapes_labels_sample)\n",
    "cityscapes_label_path_sample = np.array(cityscapes_label_path_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ecbe031c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(sample_number):\n",
    "    name = cityscapes_names_sample[i]\n",
    "    label = cityscapes_labels_sample[i]\n",
    "    label_color = class_to_color(label)\n",
    "    label_image = Image.fromarray(label_color.astype(np.uint8))\n",
    "    label_image.save(cityscpaes_save_labels_path+\"\\\\\"+name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78b74321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cityscapes_label_path_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c037ca62",
   "metadata": {},
   "source": [
    "Synthia labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fee9ca62",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthia_label_original_folder = \"D:\\zsh\\graduation\\Graduation-project-domain-shift-image-2-image\\dataset\\SYNTHIA_256\\label-rgb\"\n",
    "synthia_save_labels_path = \"D:\\zsh\\graduation\\Graduation-project-domain-shift-image-2-image\\dataset\\SYNTHIA_256\\\\new_labels\"\n",
    "synthia_label_folder_relative = os.path.relpath(synthia_save_labels_path, start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "db462624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def transform_masks_synthia(labels):\n",
    "#     masks = []\n",
    "#     for label in labels:\n",
    "#         mask = color_to_class(label)\n",
    "#         masks.append(mask)\n",
    "#     masks = np.array(masks)\n",
    "\n",
    "#     return masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4450d4fc",
   "metadata": {},
   "source": [
    "Note: comment out the corresponding lines if loading the synthia_labels_sample.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "144ebc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this usually takes a long time\n",
    "synthia_labels_sample= []\n",
    "synthia_label_path_sample = []\n",
    "\n",
    "for name in synthia_names_sample:\n",
    "    # transform the color encoding from 20 classes to 6 classes\n",
    "    original_label_rgb = Image.open(synthia_label_original_folder+\"\\\\\"+name).convert(\"RGB\")\n",
    "    original_label_rgb = np.array(original_label_rgb)\n",
    "    label = color_to_class(original_label_rgb)\n",
    "    resized_label_rgb = class_to_color(label)\n",
    "    label_path =synthia_label_folder_relative+\"\\\\\"+name\n",
    "    resized_label_rgb = Image.fromarray(resized_label_rgb.astype(np.uint8))\n",
    "    \n",
    "#     save the new labels\n",
    "    resized_label_rgb.save(synthia_save_labels_path+\"\\\\\"+name)\n",
    "    \n",
    "    synthia_labels_sample.append(label)\n",
    "    synthia_label_path_sample.append(label_path)\n",
    "\n",
    "synthia_labels_sample = np.array(synthia_labels_sample)\n",
    "synthia_label_path_sample = np.array(synthia_label_path_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "153d901b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(synthia_label_path_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0093a29",
   "metadata": {},
   "source": [
    "save the synthia masks in a pickle to save effort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "314c6cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('synthia_labels_sample.pkl', 'wb') as file:\n",
    "      \n",
    "#     # A new file will be created\n",
    "#     pickle.dump(synthia_labels_sample, file)\n",
    "\n",
    "# with open('synthia_labels_sample.pkl', 'rb') as f:\n",
    "#     synthia_labels_sample = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "302d8aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 256, 256)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthia_labels_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1540c1",
   "metadata": {},
   "source": [
    "Combine cityscapes_label_path_sample and synthia_label_path_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ca7f0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_path_sample = np.concatenate((cityscapes_label_path_sample,synthia_label_path_sample),axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b9cf3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label_path\"] = label_path_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c0884c53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>image_path</th>\n",
       "      <th>dataset</th>\n",
       "      <th>label_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>237.jpeg</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\237.jpeg</td>\n",
       "      <td>Cityscapes</td>\n",
       "      <td>dataset\\cityscapes_labels_sample\\237.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>486.jpeg</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\486.jpeg</td>\n",
       "      <td>Cityscapes</td>\n",
       "      <td>dataset\\cityscapes_labels_sample\\486.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>134.jpeg</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\134.jpeg</td>\n",
       "      <td>Cityscapes</td>\n",
       "      <td>dataset\\cityscapes_labels_sample\\134.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>442.jpeg</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\442.jpeg</td>\n",
       "      <td>Cityscapes</td>\n",
       "      <td>dataset\\cityscapes_labels_sample\\442.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>183.jpeg</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\183.jpeg</td>\n",
       "      <td>Cityscapes</td>\n",
       "      <td>dataset\\cityscapes_labels_sample\\183.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0001480.png</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0001480.png</td>\n",
       "      <td>Synthia</td>\n",
       "      <td>dataset\\SYNTHIA_256\\new_labels\\0001480.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0003215.png</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0003215.png</td>\n",
       "      <td>Synthia</td>\n",
       "      <td>dataset\\SYNTHIA_256\\new_labels\\0003215.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0002457.png</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0002457.png</td>\n",
       "      <td>Synthia</td>\n",
       "      <td>dataset\\SYNTHIA_256\\new_labels\\0002457.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0004959.png</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0004959.png</td>\n",
       "      <td>Synthia</td>\n",
       "      <td>dataset\\SYNTHIA_256\\new_labels\\0004959.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0001305.png</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0001305.png</td>\n",
       "      <td>Synthia</td>\n",
       "      <td>dataset\\SYNTHIA_256\\new_labels\\0001305.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           name                                   image_path     dataset  \\\n",
       "5      237.jpeg  dataset\\original_cityscapes_inputs\\237.jpeg  Cityscapes   \n",
       "6      486.jpeg  dataset\\original_cityscapes_inputs\\486.jpeg  Cityscapes   \n",
       "7      134.jpeg  dataset\\original_cityscapes_inputs\\134.jpeg  Cityscapes   \n",
       "8      442.jpeg  dataset\\original_cityscapes_inputs\\442.jpeg  Cityscapes   \n",
       "9      183.jpeg  dataset\\original_cityscapes_inputs\\183.jpeg  Cityscapes   \n",
       "10  0001480.png        dataset\\SYNTHIA_256\\image\\0001480.png     Synthia   \n",
       "11  0003215.png        dataset\\SYNTHIA_256\\image\\0003215.png     Synthia   \n",
       "12  0002457.png        dataset\\SYNTHIA_256\\image\\0002457.png     Synthia   \n",
       "13  0004959.png        dataset\\SYNTHIA_256\\image\\0004959.png     Synthia   \n",
       "14  0001305.png        dataset\\SYNTHIA_256\\image\\0001305.png     Synthia   \n",
       "\n",
       "                                    label_path  \n",
       "5    dataset\\cityscapes_labels_sample\\237.jpeg  \n",
       "6    dataset\\cityscapes_labels_sample\\486.jpeg  \n",
       "7    dataset\\cityscapes_labels_sample\\134.jpeg  \n",
       "8    dataset\\cityscapes_labels_sample\\442.jpeg  \n",
       "9    dataset\\cityscapes_labels_sample\\183.jpeg  \n",
       "10  dataset\\SYNTHIA_256\\new_labels\\0001480.png  \n",
       "11  dataset\\SYNTHIA_256\\new_labels\\0003215.png  \n",
       "12  dataset\\SYNTHIA_256\\new_labels\\0002457.png  \n",
       "13  dataset\\SYNTHIA_256\\new_labels\\0004959.png  \n",
       "14  dataset\\SYNTHIA_256\\new_labels\\0001305.png  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[5:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c4bd44",
   "metadata": {},
   "source": [
    "# similar_image_paths，similar_IoU_score \n",
    "(Note: called IoU_score originally): finding the most similar masks from another dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "04b12eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the most similar mask from another dataset to the current image mask\n",
    "def most_similar_mask(instance, data,mode = \"IoU\"):\n",
    "#     nodes = np.asarray(nodes)\n",
    "    # Euclidean distance calculation\n",
    "    if mode == \"IoU\":\n",
    "        iou_list = []\n",
    "        for new_instance in data:\n",
    "#             the original IoU methods were not good because when both class is 0, the intersection won't count it\n",
    "#             intersection = np.logical_and(instance, new_instance)\n",
    "#             union = np.logical_or(instance, new_instance)\n",
    "            intersection = len(np.where(instance == new_instance)[0])\n",
    "            union = instance.shape[0]*instance.shape[1]\n",
    "#             print(intersection)\n",
    "#             print(union)\n",
    "            iou_score = intersection / union\n",
    "            iou_list.append(iou_score)\n",
    "        iou_arr = np.array(iou_list)\n",
    "        best_index = np.argmax(iou_arr)\n",
    "        best_score = iou_arr[best_index]\n",
    "    \n",
    "    return best_index, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cd00a88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all the corresponding paths and similar scores for the most similar masks of the CITYSCAPES images\n",
    "cityscapes_similar_image_paths = []\n",
    "cityscapes_similar_scores = []\n",
    "\n",
    "for label in cityscapes_labels_sample:\n",
    "    image_index_synthia,similar_score = most_similar_mask(label,synthia_labels_sample)\n",
    "    similar_image_path = synthia_initial_info[image_index_synthia][1]\n",
    "    \n",
    "    cityscapes_similar_image_paths.append(similar_image_path)\n",
    "    cityscapes_similar_scores.append(similar_score)\n",
    "\n",
    "cityscapes_similar_image_paths = np.array(cityscapes_similar_image_paths)\n",
    "cityscapes_similar_scores = np.array(cityscapes_similar_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "88bee047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all the corresponding paths and similar scores for the most similar masks of the SYNTHIA images\n",
    "synthia_similar_image_paths = []\n",
    "synthia_similar_scores = []\n",
    "\n",
    "for label in synthia_labels_sample:\n",
    "    image_index_cityscapes,similar_score = most_similar_mask(label,cityscapes_labels_sample)\n",
    "    similar_image_path = cityscapes_initial_info[image_index_cityscapes][1]\n",
    "    \n",
    "    synthia_similar_image_paths.append(similar_image_path)\n",
    "    synthia_similar_scores.append(similar_score)\n",
    "\n",
    "synthia_similar_image_paths = np.array(synthia_similar_image_paths)\n",
    "synthia_similar_scores = np.array(synthia_similar_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7b8098b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_image_paths =  np.concatenate((cityscapes_similar_image_paths,synthia_similar_image_paths),axis = 0)\n",
    "similar_scores = np.concatenate((cityscapes_similar_scores,synthia_similar_scores),axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bdb54b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"similar_image_paths\"] = similar_image_paths\n",
    "df[\"similar_IoU_score\"]=similar_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bd065c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>image_path</th>\n",
       "      <th>dataset</th>\n",
       "      <th>label_path</th>\n",
       "      <th>similar_image_paths</th>\n",
       "      <th>similar_IoU_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>237.jpeg</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\237.jpeg</td>\n",
       "      <td>Cityscapes</td>\n",
       "      <td>dataset\\cityscapes_labels_sample\\237.jpeg</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0001480.png</td>\n",
       "      <td>0.435898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>486.jpeg</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\486.jpeg</td>\n",
       "      <td>Cityscapes</td>\n",
       "      <td>dataset\\cityscapes_labels_sample\\486.jpeg</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0004946.png</td>\n",
       "      <td>0.474365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>134.jpeg</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\134.jpeg</td>\n",
       "      <td>Cityscapes</td>\n",
       "      <td>dataset\\cityscapes_labels_sample\\134.jpeg</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0004946.png</td>\n",
       "      <td>0.451935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>442.jpeg</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\442.jpeg</td>\n",
       "      <td>Cityscapes</td>\n",
       "      <td>dataset\\cityscapes_labels_sample\\442.jpeg</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0001305.png</td>\n",
       "      <td>0.362625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>183.jpeg</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\183.jpeg</td>\n",
       "      <td>Cityscapes</td>\n",
       "      <td>dataset\\cityscapes_labels_sample\\183.jpeg</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0004946.png</td>\n",
       "      <td>0.359467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0001480.png</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0001480.png</td>\n",
       "      <td>Synthia</td>\n",
       "      <td>dataset\\SYNTHIA_256\\new_labels\\0001480.png</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\237.jpeg</td>\n",
       "      <td>0.435898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0003215.png</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0003215.png</td>\n",
       "      <td>Synthia</td>\n",
       "      <td>dataset\\SYNTHIA_256\\new_labels\\0003215.png</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\134.jpeg</td>\n",
       "      <td>0.390427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0002457.png</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0002457.png</td>\n",
       "      <td>Synthia</td>\n",
       "      <td>dataset\\SYNTHIA_256\\new_labels\\0002457.png</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\189.jpeg</td>\n",
       "      <td>0.336685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0004959.png</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0004959.png</td>\n",
       "      <td>Synthia</td>\n",
       "      <td>dataset\\SYNTHIA_256\\new_labels\\0004959.png</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\189.jpeg</td>\n",
       "      <td>0.478058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0001305.png</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0001305.png</td>\n",
       "      <td>Synthia</td>\n",
       "      <td>dataset\\SYNTHIA_256\\new_labels\\0001305.png</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\442.jpeg</td>\n",
       "      <td>0.362625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           name                                   image_path     dataset  \\\n",
       "5      237.jpeg  dataset\\original_cityscapes_inputs\\237.jpeg  Cityscapes   \n",
       "6      486.jpeg  dataset\\original_cityscapes_inputs\\486.jpeg  Cityscapes   \n",
       "7      134.jpeg  dataset\\original_cityscapes_inputs\\134.jpeg  Cityscapes   \n",
       "8      442.jpeg  dataset\\original_cityscapes_inputs\\442.jpeg  Cityscapes   \n",
       "9      183.jpeg  dataset\\original_cityscapes_inputs\\183.jpeg  Cityscapes   \n",
       "10  0001480.png        dataset\\SYNTHIA_256\\image\\0001480.png     Synthia   \n",
       "11  0003215.png        dataset\\SYNTHIA_256\\image\\0003215.png     Synthia   \n",
       "12  0002457.png        dataset\\SYNTHIA_256\\image\\0002457.png     Synthia   \n",
       "13  0004959.png        dataset\\SYNTHIA_256\\image\\0004959.png     Synthia   \n",
       "14  0001305.png        dataset\\SYNTHIA_256\\image\\0001305.png     Synthia   \n",
       "\n",
       "                                    label_path  \\\n",
       "5    dataset\\cityscapes_labels_sample\\237.jpeg   \n",
       "6    dataset\\cityscapes_labels_sample\\486.jpeg   \n",
       "7    dataset\\cityscapes_labels_sample\\134.jpeg   \n",
       "8    dataset\\cityscapes_labels_sample\\442.jpeg   \n",
       "9    dataset\\cityscapes_labels_sample\\183.jpeg   \n",
       "10  dataset\\SYNTHIA_256\\new_labels\\0001480.png   \n",
       "11  dataset\\SYNTHIA_256\\new_labels\\0003215.png   \n",
       "12  dataset\\SYNTHIA_256\\new_labels\\0002457.png   \n",
       "13  dataset\\SYNTHIA_256\\new_labels\\0004959.png   \n",
       "14  dataset\\SYNTHIA_256\\new_labels\\0001305.png   \n",
       "\n",
       "                            similar_image_paths  similar_IoU_score  \n",
       "5         dataset\\SYNTHIA_256\\image\\0001480.png           0.435898  \n",
       "6         dataset\\SYNTHIA_256\\image\\0004946.png           0.474365  \n",
       "7         dataset\\SYNTHIA_256\\image\\0004946.png           0.451935  \n",
       "8         dataset\\SYNTHIA_256\\image\\0001305.png           0.362625  \n",
       "9         dataset\\SYNTHIA_256\\image\\0004946.png           0.359467  \n",
       "10  dataset\\original_cityscapes_inputs\\237.jpeg           0.435898  \n",
       "11  dataset\\original_cityscapes_inputs\\134.jpeg           0.390427  \n",
       "12  dataset\\original_cityscapes_inputs\\189.jpeg           0.336685  \n",
       "13  dataset\\original_cityscapes_inputs\\189.jpeg           0.478058  \n",
       "14  dataset\\original_cityscapes_inputs\\442.jpeg           0.362625  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.iloc[495:505]\n",
    "df.iloc[5:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fc7e6f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"system_df_full.csv\",index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404e1a7d",
   "metadata": {},
   "source": [
    "# generate from list of images and labels\n",
    "\n",
    "* class distribution\n",
    "    * (other_ratio, road_ratio, sidewalk_ratio, vegetation_ratio, sky_ratio, car_ratio)\n",
    "* embedding of input (originally: tsne_1, tsne_2)\n",
    "    * simple dimensionality reduction (simple_tsne_1,simple_tsne_2)\n",
    "    * and also the embedding from classification model(meaningful_tsne_1,meaningful_tsne_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aba43f",
   "metadata": {},
   "source": [
    "Load images to data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "506190eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityscapesDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, images, labels, noise_level = 0):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.noise_level = noise_level\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        label = self.labels[index]\n",
    "        # helps to change the noise level\n",
    "        if self.noise_level!=0:\n",
    "            image = image+(self.noise_level*np.random.normal(0, (image.max() - image.min()), image.shape)).astype(\"uint8\") # (mean, sigma, image_shape)\n",
    "        image = self.transform(image)\n",
    "        label = torch.from_numpy(label).long()\n",
    "        return image, label\n",
    "        \n",
    "    def transform(self, image):\n",
    "        # normalization\n",
    "        transform_ops = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)) # normalize to control the \"dynamic range\" of activations of different layers\n",
    "        ])\n",
    "        return transform_ops(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d83df314",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = \"D:\\zsh\\graduation\\\\Graduation-project-domain-shift-image-2-image\"\n",
    "# cityscapes_label_path = \"dataset\\\\cityscapes_labels_100\"\n",
    "cityscapes_images = []\n",
    "cityscapes_labels = []\n",
    "synthia_images = []\n",
    "synthia_labels = []\n",
    "\n",
    "# get the corresponding images and labels for the dataloader\n",
    "for index,path in enumerate(df[\"image_path\"]):\n",
    "    full_path = os.path.join(repo_path,path)\n",
    "    dataset = df[\"dataset\"].iloc[index]\n",
    "    # parse to get the name of image\n",
    "    name = os.path.split(path)[1]\n",
    "    label_index = int(name.split(\".\")[0])\n",
    "    if dataset == \"Cityscapes\":\n",
    "        # image\n",
    "        img = Image.open(full_path)\n",
    "        img = np.array(img)\n",
    "        # label\n",
    "        label = Y_test[label_index]\n",
    "        label = np.array(label)\n",
    "        cityscapes_images.append(img)\n",
    "        cityscapes_labels.append(label)\n",
    "    elif dataset == \"Synthia\":\n",
    "        # image\n",
    "        img = Image.open(full_path)\n",
    "        img = np.array(img)\n",
    "        # label\n",
    "        label = synthia_labels_sample[index-sample_number]\n",
    "        label = np.array(label)\n",
    "        synthia_images.append(img)\n",
    "        synthia_labels.append(label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b0c53def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 256, 256, 3)\n",
      "(10, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "cityscapes_images = np.array(cityscapes_images)\n",
    "cityscapes_labels = np.array(cityscapes_labels)\n",
    "synthia_images = np.array(synthia_images)\n",
    "synthia_labels = np.array(synthia_labels)\n",
    "\n",
    "print(synthia_images.shape)\n",
    "print(synthia_images.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3655394d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load to Dataset class and DataLoader\n",
    "batch_size = 1\n",
    "cityscapes_dataset = CityscapesDataset(cityscapes_images, cityscapes_labels,noise_level=0) # no noise for this comparison\n",
    "cityscapes_loader = DataLoader(cityscapes_dataset, batch_size=batch_size)\n",
    "\n",
    "synthia_dataset = CityscapesDataset(synthia_images, synthia_labels,noise_level=0)\n",
    "synthia_loader = DataLoader(synthia_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3cfb1a",
   "metadata": {},
   "source": [
    "**class distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "31080428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the ratio for each class\n",
    "def get_class_distribution(labels):\n",
    "    class_dist = []\n",
    "    for label in labels:\n",
    "        class_for_label = []\n",
    "        for i in range(6):\n",
    "            element_count = np.count_nonzero(label==i)\n",
    "            class_for_label.append(element_count/(256*256))\n",
    "        class_dist.append(class_for_label)\n",
    "        class_for_label= class_for_label# change to another size if the image size is changed\n",
    "    class_dist = np.array(class_dist)\n",
    "    return class_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dbac5c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cityscpaes_class_dist = get_class_distribution(cityscapes_labels)\n",
    "# synthia_labels\n",
    "synthia_class_dist = get_class_distribution(synthia_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c126f830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 6)\n",
      "(10, 6)\n"
     ]
    }
   ],
   "source": [
    "print(cityscpaes_class_dist.shape)\n",
    "print(synthia_class_dist.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "04886605",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dist = np.concatenate((cityscpaes_class_dist,synthia_class_dist),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6f6c94e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>other_ratio</th>\n",
       "      <th>road_ratio</th>\n",
       "      <th>sidewalk_ratio</th>\n",
       "      <th>vegetation_ratio</th>\n",
       "      <th>sky_ratio</th>\n",
       "      <th>car_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.268173</td>\n",
       "      <td>0.380219</td>\n",
       "      <td>0.054855</td>\n",
       "      <td>0.259460</td>\n",
       "      <td>0.026566</td>\n",
       "      <td>0.010727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.390427</td>\n",
       "      <td>0.322311</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.128143</td>\n",
       "      <td>0.004623</td>\n",
       "      <td>0.154495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.615875</td>\n",
       "      <td>0.264603</td>\n",
       "      <td>0.010590</td>\n",
       "      <td>0.037872</td>\n",
       "      <td>0.023697</td>\n",
       "      <td>0.047363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.235931</td>\n",
       "      <td>0.296295</td>\n",
       "      <td>0.021698</td>\n",
       "      <td>0.421646</td>\n",
       "      <td>0.023544</td>\n",
       "      <td>0.000885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.501968</td>\n",
       "      <td>0.312622</td>\n",
       "      <td>0.026047</td>\n",
       "      <td>0.157547</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.001785</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   other_ratio  road_ratio  sidewalk_ratio  vegetation_ratio  sky_ratio  \\\n",
       "0     0.268173    0.380219        0.054855          0.259460   0.026566   \n",
       "1     0.390427    0.322311        0.000000          0.128143   0.004623   \n",
       "2     0.615875    0.264603        0.010590          0.037872   0.023697   \n",
       "3     0.235931    0.296295        0.021698          0.421646   0.023544   \n",
       "4     0.501968    0.312622        0.026047          0.157547   0.000031   \n",
       "\n",
       "   car_ratio  \n",
       "0   0.010727  \n",
       "1   0.154495  \n",
       "2   0.047363  \n",
       "3   0.000885  \n",
       "4   0.001785  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_dist_df = pd.DataFrame(class_dist,columns=[\"other_ratio\",\"road_ratio\",\"sidewalk_ratio\",\"vegetation_ratio\",\"sky_ratio\",\"car_ratio\"])\n",
    "class_dist_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "93077db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.concat((df,class_dist_df),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336b82ef",
   "metadata": {},
   "source": [
    "**embedding of input**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c62f4a5",
   "metadata": {},
   "source": [
    "* simple dimensionality reduction (simple_tsne_1,simple_tsne_2)\n",
    "* and also the embedding from classification model(meaningful_tsne_1,meaningful_tsne_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3575f8",
   "metadata": {},
   "source": [
    "**TODO: change the two tsne to combined with PCA, if necessary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7486fd3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.412335609776955\n"
     ]
    }
   ],
   "source": [
    "# simple dimensionality reduction\n",
    "combined_images = np.concatenate((cityscapes_images,synthia_images),axis=0)\n",
    "simple_combined_embedding = combined_images.reshape(combined_images.shape[0],-1)\n",
    "# simple_combined_embedding =np.reshape(combined_images,(len(combined_images), 256*256*3))\n",
    "\n",
    "# pca_50 = PCA(n_components=50)\n",
    "# pca_embedding = pca_50.fit_transform(simple_combined_embedding)\n",
    "# print(np.sum(pca_50.explained_variance_ratio_))\n",
    "tsne = TSNE()\n",
    "simple_tsne_embedding = tsne.fit_transform(simple_combined_embedding)\n",
    "# simple_tsne_embedding = tsne.fit_transform(pca_embedding)\n",
    "\n",
    "\n",
    "# simple pca embedding\n",
    "pca = PCA(n_components=2)\n",
    "simple_pca_embedding = pca.fit_transform(simple_combined_embedding)\n",
    "print(np.sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5819a673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\zsh\\graduation\\ViTs-vs-CNNs\n"
     ]
    }
   ],
   "source": [
    "# move to the pre-trained model path\n",
    "cd D:\\zsh\\graduation\\ViTs-vs-CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "481483ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import global_val\n",
    "\n",
    "from models.ghost_bn import GhostBN2D_ADV\n",
    "from models.advresnet_gbn_gelu import Affine\n",
    "import models.advresnet_gbn_gelu as advres\n",
    "from main_adv_res import EightBN\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load weights\n",
    "classifier_model = advres.__dict__[\"resnet50\"](norm_layer = EightBN)\n",
    "weights_path = \"D:\\zsh\\graduation\\Graduation-project-domain-shift-image-2-image\\models\\\\advres50_gelu.pth\"\n",
    "weight_dict = torch.load(weights_path,map_location=device)[\"model\"]\n",
    "\n",
    "classifier_model.load_state_dict(weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "05dadaab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\zsh\\graduation\\Graduation-project-domain-shift-image-2-image\\jupyter\n"
     ]
    }
   ],
   "source": [
    "# move back to orriginal path\n",
    "cd D:\\zsh\\graduation\\Graduation-project-domain-shift-image-2-image\\jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6a6f80f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this get the embedding from the pre-trained classification model for one image\n",
    "def get_representation_for_image(image,label):\n",
    "    # resize the image and transform it to tensor # [256,256,3] -> [224,224,3] \n",
    "    resized_image = resize(image, (224, 224,3))\n",
    "    resized_image_tensor = torch.from_numpy(resized_image)\n",
    "    resized_image_tensor = torch.permute(resized_image_tensor, (2, 0, 1))\n",
    "    resized_image_tensor = resized_image_tensor[None,:] # [3,224,224] -> [1,3,224,224] (because model takes 4D input)\n",
    "    \n",
    "    # dictionary for the activations\n",
    "    activations = {}\n",
    "\n",
    "    def get_activations(name):\n",
    "        def hook(model, input, output):\n",
    "            activations[name] = output.detach()\n",
    "        return hook\n",
    "\n",
    "    # use the output of (avgpool) layer because it is the same as the input of last layer\n",
    "    h = classifier_model.avgpool.register_forward_hook(get_activations(\"input_last_layer\"))\n",
    "\n",
    "    classifier_model.eval()\n",
    "    classifier_model.sing=True\n",
    "    classifier_model.training=False\n",
    "    out = classifier_model(resized_image_tensor.float(),label)\n",
    "    \n",
    "    # remove the hook\n",
    "    h.remove()\n",
    "\n",
    "    first_part_embedding = torch.flatten(activations[\"input_last_layer\"][0])\n",
    "    \n",
    "    return first_part_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9b7d3928",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_list = []\n",
    "for i, image in enumerate(combined_images): # i is not used, could remove the enumerate\n",
    "    first_part_embedding = get_representation_for_image(image,\"__\") # y_label is never used for the pretrained model, so just use replacement\n",
    "    second_part_embedding = class_dist[i] # second part of the embedding is class ratio\n",
    "    embedding = np.concatenate((first_part_embedding.numpy(),np.array(second_part_embedding)),axis = 0)\n",
    "    embedding_list.append(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3d21f8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_arr = np.array(embedding_list)\n",
    "# pca_50 = PCA(n_components=50)\n",
    "# pca_embedding = pca_50.fit_transform(embedding_arr)\n",
    "# print(np.sum(pca_50.explained_variance_ratio_))\n",
    "# pca = PCA(n_components=2)\n",
    "tsne = TSNE()\n",
    "# meaningful_tsne_embedding = tsne.fit_transform(pca_embedding)\n",
    "meaningful_tsne_embedding = tsne.fit_transform(embedding_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4bd3fbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_embedding = np.concatenate((simple_tsne_embedding,meaningful_tsne_embedding,simple_pca_embedding),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d8e6de70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input_embedding = pd.DataFrame(full_embedding, columns=[\"simple_tsne_1\",\"simple_tsne_2\",\"meaningful_tsne_1\",\"meaningful_tsne_2\",\"pca_1\",\"pca_2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4c64bea8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>simple_tsne_1</th>\n",
       "      <th>simple_tsne_2</th>\n",
       "      <th>meaningful_tsne_1</th>\n",
       "      <th>meaningful_tsne_2</th>\n",
       "      <th>pca_1</th>\n",
       "      <th>pca_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>61.698845</td>\n",
       "      <td>55.318829</td>\n",
       "      <td>-85.283386</td>\n",
       "      <td>-72.167946</td>\n",
       "      <td>503.546415</td>\n",
       "      <td>-3180.000910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-12.906113</td>\n",
       "      <td>21.398748</td>\n",
       "      <td>-35.235085</td>\n",
       "      <td>14.135887</td>\n",
       "      <td>7021.621696</td>\n",
       "      <td>-6708.378633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26.645414</td>\n",
       "      <td>-58.506546</td>\n",
       "      <td>-182.491776</td>\n",
       "      <td>123.563637</td>\n",
       "      <td>6927.331786</td>\n",
       "      <td>-2858.908411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101.314774</td>\n",
       "      <td>-74.139946</td>\n",
       "      <td>-197.710770</td>\n",
       "      <td>-70.297905</td>\n",
       "      <td>-1387.100221</td>\n",
       "      <td>1508.490089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69.829346</td>\n",
       "      <td>5.824750</td>\n",
       "      <td>15.701824</td>\n",
       "      <td>-76.754539</td>\n",
       "      <td>1293.309014</td>\n",
       "      <td>-6028.467934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   simple_tsne_1  simple_tsne_2  meaningful_tsne_1  meaningful_tsne_2  \\\n",
       "0      61.698845      55.318829         -85.283386         -72.167946   \n",
       "1     -12.906113      21.398748         -35.235085          14.135887   \n",
       "2      26.645414     -58.506546        -182.491776         123.563637   \n",
       "3     101.314774     -74.139946        -197.710770         -70.297905   \n",
       "4      69.829346       5.824750          15.701824         -76.754539   \n",
       "\n",
       "         pca_1        pca_2  \n",
       "0   503.546415 -3180.000910  \n",
       "1  7021.621696 -6708.378633  \n",
       "2  6927.331786 -2858.908411  \n",
       "3 -1387.100221  1508.490089  \n",
       "4  1293.309014 -6028.467934  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_input_embedding.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7623f514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>image_path</th>\n",
       "      <th>dataset</th>\n",
       "      <th>label_path</th>\n",
       "      <th>similar_image_paths</th>\n",
       "      <th>similar_IoU_score</th>\n",
       "      <th>other_ratio</th>\n",
       "      <th>road_ratio</th>\n",
       "      <th>sidewalk_ratio</th>\n",
       "      <th>vegetation_ratio</th>\n",
       "      <th>sky_ratio</th>\n",
       "      <th>car_ratio</th>\n",
       "      <th>simple_tsne_1</th>\n",
       "      <th>simple_tsne_2</th>\n",
       "      <th>meaningful_tsne_1</th>\n",
       "      <th>meaningful_tsne_2</th>\n",
       "      <th>pca_1</th>\n",
       "      <th>pca_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.jpeg</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\14.jpeg</td>\n",
       "      <td>Cityscapes</td>\n",
       "      <td>dataset\\cityscapes_labels_sample\\14.jpeg</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0001305.png</td>\n",
       "      <td>0.353790</td>\n",
       "      <td>0.268173</td>\n",
       "      <td>0.380219</td>\n",
       "      <td>0.054855</td>\n",
       "      <td>0.259460</td>\n",
       "      <td>0.026566</td>\n",
       "      <td>0.010727</td>\n",
       "      <td>61.698845</td>\n",
       "      <td>55.318829</td>\n",
       "      <td>-85.283386</td>\n",
       "      <td>-72.167946</td>\n",
       "      <td>503.546415</td>\n",
       "      <td>-3180.000910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87.jpeg</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\87.jpeg</td>\n",
       "      <td>Cityscapes</td>\n",
       "      <td>dataset\\cityscapes_labels_sample\\87.jpeg</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0004946.png</td>\n",
       "      <td>0.351639</td>\n",
       "      <td>0.390427</td>\n",
       "      <td>0.322311</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.128143</td>\n",
       "      <td>0.004623</td>\n",
       "      <td>0.154495</td>\n",
       "      <td>-12.906113</td>\n",
       "      <td>21.398748</td>\n",
       "      <td>-35.235085</td>\n",
       "      <td>14.135887</td>\n",
       "      <td>7021.621696</td>\n",
       "      <td>-6708.378633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>189.jpeg</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\189.jpeg</td>\n",
       "      <td>Cityscapes</td>\n",
       "      <td>dataset\\cityscapes_labels_sample\\189.jpeg</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0004959.png</td>\n",
       "      <td>0.478058</td>\n",
       "      <td>0.615875</td>\n",
       "      <td>0.264603</td>\n",
       "      <td>0.010590</td>\n",
       "      <td>0.037872</td>\n",
       "      <td>0.023697</td>\n",
       "      <td>0.047363</td>\n",
       "      <td>26.645414</td>\n",
       "      <td>-58.506546</td>\n",
       "      <td>-182.491776</td>\n",
       "      <td>123.563637</td>\n",
       "      <td>6927.331786</td>\n",
       "      <td>-2858.908411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>167.jpeg</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\167.jpeg</td>\n",
       "      <td>Cityscapes</td>\n",
       "      <td>dataset\\cityscapes_labels_sample\\167.jpeg</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0003215.png</td>\n",
       "      <td>0.333984</td>\n",
       "      <td>0.235931</td>\n",
       "      <td>0.296295</td>\n",
       "      <td>0.021698</td>\n",
       "      <td>0.421646</td>\n",
       "      <td>0.023544</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>101.314774</td>\n",
       "      <td>-74.139946</td>\n",
       "      <td>-197.710770</td>\n",
       "      <td>-70.297905</td>\n",
       "      <td>-1387.100221</td>\n",
       "      <td>1508.490089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>439.jpeg</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\439.jpeg</td>\n",
       "      <td>Cityscapes</td>\n",
       "      <td>dataset\\cityscapes_labels_sample\\439.jpeg</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0005761.png</td>\n",
       "      <td>0.400711</td>\n",
       "      <td>0.501968</td>\n",
       "      <td>0.312622</td>\n",
       "      <td>0.026047</td>\n",
       "      <td>0.157547</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.001785</td>\n",
       "      <td>69.829346</td>\n",
       "      <td>5.824750</td>\n",
       "      <td>15.701824</td>\n",
       "      <td>-76.754539</td>\n",
       "      <td>1293.309014</td>\n",
       "      <td>-6028.467934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       name                                   image_path     dataset  \\\n",
       "0   14.jpeg   dataset\\original_cityscapes_inputs\\14.jpeg  Cityscapes   \n",
       "1   87.jpeg   dataset\\original_cityscapes_inputs\\87.jpeg  Cityscapes   \n",
       "2  189.jpeg  dataset\\original_cityscapes_inputs\\189.jpeg  Cityscapes   \n",
       "3  167.jpeg  dataset\\original_cityscapes_inputs\\167.jpeg  Cityscapes   \n",
       "4  439.jpeg  dataset\\original_cityscapes_inputs\\439.jpeg  Cityscapes   \n",
       "\n",
       "                                  label_path  \\\n",
       "0   dataset\\cityscapes_labels_sample\\14.jpeg   \n",
       "1   dataset\\cityscapes_labels_sample\\87.jpeg   \n",
       "2  dataset\\cityscapes_labels_sample\\189.jpeg   \n",
       "3  dataset\\cityscapes_labels_sample\\167.jpeg   \n",
       "4  dataset\\cityscapes_labels_sample\\439.jpeg   \n",
       "\n",
       "                     similar_image_paths  similar_IoU_score  other_ratio  \\\n",
       "0  dataset\\SYNTHIA_256\\image\\0001305.png           0.353790     0.268173   \n",
       "1  dataset\\SYNTHIA_256\\image\\0004946.png           0.351639     0.390427   \n",
       "2  dataset\\SYNTHIA_256\\image\\0004959.png           0.478058     0.615875   \n",
       "3  dataset\\SYNTHIA_256\\image\\0003215.png           0.333984     0.235931   \n",
       "4  dataset\\SYNTHIA_256\\image\\0005761.png           0.400711     0.501968   \n",
       "\n",
       "   road_ratio  sidewalk_ratio  vegetation_ratio  sky_ratio  car_ratio  \\\n",
       "0    0.380219        0.054855          0.259460   0.026566   0.010727   \n",
       "1    0.322311        0.000000          0.128143   0.004623   0.154495   \n",
       "2    0.264603        0.010590          0.037872   0.023697   0.047363   \n",
       "3    0.296295        0.021698          0.421646   0.023544   0.000885   \n",
       "4    0.312622        0.026047          0.157547   0.000031   0.001785   \n",
       "\n",
       "   simple_tsne_1  simple_tsne_2  meaningful_tsne_1  meaningful_tsne_2  \\\n",
       "0      61.698845      55.318829         -85.283386         -72.167946   \n",
       "1     -12.906113      21.398748         -35.235085          14.135887   \n",
       "2      26.645414     -58.506546        -182.491776         123.563637   \n",
       "3     101.314774     -74.139946        -197.710770         -70.297905   \n",
       "4      69.829346       5.824750          15.701824         -76.754539   \n",
       "\n",
       "         pca_1        pca_2  \n",
       "0   503.546415 -3180.000910  \n",
       "1  7021.621696 -6708.378633  \n",
       "2  6927.331786 -2858.908411  \n",
       "3 -1387.100221  1508.490089  \n",
       "4  1293.309014 -6028.467934  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full = pd.concat((df_full,df_input_embedding),axis = 1)\n",
    "df_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "da7abc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.to_csv(\"system_df_small_sample.csv\",index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397872e3",
   "metadata": {},
   "source": [
    "#  bottleneck_activations_embedding, prediction_path, and performance\n",
    "* save output locally, and then add path (prediction_path)\n",
    "* performance: (other_IoU, road_IoU, sidewalk_IoU, vegetation_IoU, sky_IoU, car_IoU)\n",
    "    **How do you get the IoU per class (check original paper)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671874a6",
   "metadata": {},
   "source": [
    "Read the data (for updating the bottleneck activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9ffd2f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_full.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2997f0e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>image_path</th>\n",
       "      <th>dataset</th>\n",
       "      <th>label_path</th>\n",
       "      <th>similar_image_paths</th>\n",
       "      <th>similar_IoU_score</th>\n",
       "      <th>other_ratio</th>\n",
       "      <th>road_ratio</th>\n",
       "      <th>sidewalk_ratio</th>\n",
       "      <th>vegetation_ratio</th>\n",
       "      <th>sky_ratio</th>\n",
       "      <th>car_ratio</th>\n",
       "      <th>simple_tsne_1</th>\n",
       "      <th>simple_tsne_2</th>\n",
       "      <th>meaningful_tsne_1</th>\n",
       "      <th>meaningful_tsne_2</th>\n",
       "      <th>pca_1</th>\n",
       "      <th>pca_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.jpeg</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\14.jpeg</td>\n",
       "      <td>Cityscapes</td>\n",
       "      <td>dataset\\cityscapes_labels_sample\\14.jpeg</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0001305.png</td>\n",
       "      <td>0.353790</td>\n",
       "      <td>0.268173</td>\n",
       "      <td>0.380219</td>\n",
       "      <td>0.054855</td>\n",
       "      <td>0.259460</td>\n",
       "      <td>0.026566</td>\n",
       "      <td>0.010727</td>\n",
       "      <td>61.698845</td>\n",
       "      <td>55.318829</td>\n",
       "      <td>-85.283386</td>\n",
       "      <td>-72.167946</td>\n",
       "      <td>503.546415</td>\n",
       "      <td>-3180.000910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87.jpeg</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\87.jpeg</td>\n",
       "      <td>Cityscapes</td>\n",
       "      <td>dataset\\cityscapes_labels_sample\\87.jpeg</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0004946.png</td>\n",
       "      <td>0.351639</td>\n",
       "      <td>0.390427</td>\n",
       "      <td>0.322311</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.128143</td>\n",
       "      <td>0.004623</td>\n",
       "      <td>0.154495</td>\n",
       "      <td>-12.906113</td>\n",
       "      <td>21.398748</td>\n",
       "      <td>-35.235085</td>\n",
       "      <td>14.135887</td>\n",
       "      <td>7021.621696</td>\n",
       "      <td>-6708.378633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>189.jpeg</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\189.jpeg</td>\n",
       "      <td>Cityscapes</td>\n",
       "      <td>dataset\\cityscapes_labels_sample\\189.jpeg</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0004959.png</td>\n",
       "      <td>0.478058</td>\n",
       "      <td>0.615875</td>\n",
       "      <td>0.264603</td>\n",
       "      <td>0.010590</td>\n",
       "      <td>0.037872</td>\n",
       "      <td>0.023697</td>\n",
       "      <td>0.047363</td>\n",
       "      <td>26.645414</td>\n",
       "      <td>-58.506546</td>\n",
       "      <td>-182.491776</td>\n",
       "      <td>123.563637</td>\n",
       "      <td>6927.331786</td>\n",
       "      <td>-2858.908411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>167.jpeg</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\167.jpeg</td>\n",
       "      <td>Cityscapes</td>\n",
       "      <td>dataset\\cityscapes_labels_sample\\167.jpeg</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0003215.png</td>\n",
       "      <td>0.333984</td>\n",
       "      <td>0.235931</td>\n",
       "      <td>0.296295</td>\n",
       "      <td>0.021698</td>\n",
       "      <td>0.421646</td>\n",
       "      <td>0.023544</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>101.314774</td>\n",
       "      <td>-74.139946</td>\n",
       "      <td>-197.710770</td>\n",
       "      <td>-70.297905</td>\n",
       "      <td>-1387.100221</td>\n",
       "      <td>1508.490089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>439.jpeg</td>\n",
       "      <td>dataset\\original_cityscapes_inputs\\439.jpeg</td>\n",
       "      <td>Cityscapes</td>\n",
       "      <td>dataset\\cityscapes_labels_sample\\439.jpeg</td>\n",
       "      <td>dataset\\SYNTHIA_256\\image\\0005761.png</td>\n",
       "      <td>0.400711</td>\n",
       "      <td>0.501968</td>\n",
       "      <td>0.312622</td>\n",
       "      <td>0.026047</td>\n",
       "      <td>0.157547</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.001785</td>\n",
       "      <td>69.829346</td>\n",
       "      <td>5.824750</td>\n",
       "      <td>15.701824</td>\n",
       "      <td>-76.754539</td>\n",
       "      <td>1293.309014</td>\n",
       "      <td>-6028.467934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       name                                   image_path     dataset  \\\n",
       "0   14.jpeg   dataset\\original_cityscapes_inputs\\14.jpeg  Cityscapes   \n",
       "1   87.jpeg   dataset\\original_cityscapes_inputs\\87.jpeg  Cityscapes   \n",
       "2  189.jpeg  dataset\\original_cityscapes_inputs\\189.jpeg  Cityscapes   \n",
       "3  167.jpeg  dataset\\original_cityscapes_inputs\\167.jpeg  Cityscapes   \n",
       "4  439.jpeg  dataset\\original_cityscapes_inputs\\439.jpeg  Cityscapes   \n",
       "\n",
       "                                  label_path  \\\n",
       "0   dataset\\cityscapes_labels_sample\\14.jpeg   \n",
       "1   dataset\\cityscapes_labels_sample\\87.jpeg   \n",
       "2  dataset\\cityscapes_labels_sample\\189.jpeg   \n",
       "3  dataset\\cityscapes_labels_sample\\167.jpeg   \n",
       "4  dataset\\cityscapes_labels_sample\\439.jpeg   \n",
       "\n",
       "                     similar_image_paths  similar_IoU_score  other_ratio  \\\n",
       "0  dataset\\SYNTHIA_256\\image\\0001305.png           0.353790     0.268173   \n",
       "1  dataset\\SYNTHIA_256\\image\\0004946.png           0.351639     0.390427   \n",
       "2  dataset\\SYNTHIA_256\\image\\0004959.png           0.478058     0.615875   \n",
       "3  dataset\\SYNTHIA_256\\image\\0003215.png           0.333984     0.235931   \n",
       "4  dataset\\SYNTHIA_256\\image\\0005761.png           0.400711     0.501968   \n",
       "\n",
       "   road_ratio  sidewalk_ratio  vegetation_ratio  sky_ratio  car_ratio  \\\n",
       "0    0.380219        0.054855          0.259460   0.026566   0.010727   \n",
       "1    0.322311        0.000000          0.128143   0.004623   0.154495   \n",
       "2    0.264603        0.010590          0.037872   0.023697   0.047363   \n",
       "3    0.296295        0.021698          0.421646   0.023544   0.000885   \n",
       "4    0.312622        0.026047          0.157547   0.000031   0.001785   \n",
       "\n",
       "   simple_tsne_1  simple_tsne_2  meaningful_tsne_1  meaningful_tsne_2  \\\n",
       "0      61.698845      55.318829         -85.283386         -72.167946   \n",
       "1     -12.906113      21.398748         -35.235085          14.135887   \n",
       "2      26.645414     -58.506546        -182.491776         123.563637   \n",
       "3     101.314774     -74.139946        -197.710770         -70.297905   \n",
       "4      69.829346       5.824750          15.701824         -76.754539   \n",
       "\n",
       "         pca_1        pca_2  \n",
       "0   503.546415 -3180.000910  \n",
       "1  7021.621696 -6708.378633  \n",
       "2  6927.331786 -2858.908411  \n",
       "3 -1387.100221  1508.490089  \n",
       "4  1293.309014 -6028.467934  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c753f699",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c6d608ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model structure of U-Net\n",
    "class UNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes):\n",
    "        super(UNet, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.contracting_11 = self.conv_block(in_channels=3, out_channels=64)\n",
    "        self.contracting_12 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.contracting_21 = self.conv_block(in_channels=64, out_channels=128)\n",
    "        self.contracting_22 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.contracting_31 = self.conv_block(in_channels=128, out_channels=256)\n",
    "        self.contracting_32 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.contracting_41 = self.conv_block(in_channels=256, out_channels=512)\n",
    "        self.contracting_42 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.middle = self.conv_block(in_channels=512, out_channels=1024)\n",
    "        self.expansive_11 = nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.expansive_12 = self.conv_block(in_channels=1024, out_channels=512)\n",
    "        self.expansive_21 = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.expansive_22 = self.conv_block(in_channels=512, out_channels=256)\n",
    "        self.expansive_31 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.expansive_32 = self.conv_block(in_channels=256, out_channels=128)\n",
    "        self.expansive_41 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.expansive_42 = self.conv_block(in_channels=128, out_channels=64)\n",
    "        self.output = nn.Conv2d(in_channels=64, out_channels=num_classes, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        block = nn.Sequential(nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.BatchNorm2d(num_features=out_channels),\n",
    "                                    nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.BatchNorm2d(num_features=out_channels))\n",
    "        return block\n",
    "    \n",
    "    def forward(self, X):\n",
    "        contracting_11_out = self.contracting_11(X) # [-1, 64, 256, 256]\n",
    "        contracting_12_out = self.contracting_12(contracting_11_out) # [-1, 64, 128, 128]\n",
    "        contracting_21_out = self.contracting_21(contracting_12_out) # [-1, 128, 128, 128]\n",
    "        contracting_22_out = self.contracting_22(contracting_21_out) # [-1, 128, 64, 64]\n",
    "        contracting_31_out = self.contracting_31(contracting_22_out) # [-1, 256, 64, 64]\n",
    "        contracting_32_out = self.contracting_32(contracting_31_out) # [-1, 256, 32, 32]\n",
    "        contracting_41_out = self.contracting_41(contracting_32_out) # [-1, 512, 32, 32]\n",
    "        contracting_42_out = self.contracting_42(contracting_41_out) # [-1, 512, 16, 16]\n",
    "        middle_out = self.middle(contracting_42_out) # [-1, 1024, 16, 16]\n",
    "        expansive_11_out = self.expansive_11(middle_out) # [-1, 512, 32, 32]\n",
    "        expansive_12_out = self.expansive_12(torch.cat((expansive_11_out, contracting_41_out), dim=1)) # [-1, 1024, 32, 32] -> [-1, 512, 32, 32]\n",
    "        expansive_21_out = self.expansive_21(expansive_12_out) # [-1, 256, 64, 64]\n",
    "        expansive_22_out = self.expansive_22(torch.cat((expansive_21_out, contracting_31_out), dim=1)) # [-1, 512, 64, 64] -> [-1, 256, 64, 64]\n",
    "        expansive_31_out = self.expansive_31(expansive_22_out) # [-1, 128, 128, 128]\n",
    "        expansive_32_out = self.expansive_32(torch.cat((expansive_31_out, contracting_21_out), dim=1)) # [-1, 256, 128, 128] -> [-1, 128, 128, 128]\n",
    "        expansive_41_out = self.expansive_41(expansive_32_out) # [-1, 64, 256, 256]\n",
    "        expansive_42_out = self.expansive_42(torch.cat((expansive_41_out, contracting_11_out), dim=1)) # [-1, 128, 256, 256] -> [-1, 64, 256, 256]\n",
    "        output_out = self.output(expansive_42_out) # [-1, num_classes, 256, 256]\n",
    "        return output_out,middle_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ff718676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"D:\\zsh\\graduation\\Graduation-project-domain-shift-image-2-image\\models\\\\5-classes-U-Net-2023-03-09.pth\"\n",
    "\n",
    "device = torch.device('cpu')\n",
    "model_ = UNet(num_classes=6).to(device)\n",
    "model_.load_state_dict(torch.load(model_path,map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "bbd6ac2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IoU_per_class(prediction,label):\n",
    "    prediction = prediction.detach().numpy()\n",
    "    label = label.detach().numpy()\n",
    "    # Loop over each class\n",
    "    iou_list = []\n",
    "    # Flatten label and class arrays\n",
    "    flat_prediction = prediction.flatten()\n",
    "    flat_label = label.flatten()\n",
    "\n",
    "    for i in range(6):\n",
    "        \n",
    "        # Calculate intersection and union\n",
    "        intersection = np.sum((flat_prediction == i) & (flat_label == i))\n",
    "        union = np.sum((flat_prediction == i) | (flat_label == i))\n",
    "\n",
    "        # Calculate IoU\n",
    "        iou = intersection / (union + 1e-12) # prevent infinitiy iou value\n",
    "\n",
    "        # Store IoU value in dictionary\n",
    "        iou_list.append(iou)\n",
    "    \n",
    "    # calculate overall IoU\n",
    "    intersection = len(np.where(flat_prediction == flat_label)[0])\n",
    "    union = len(flat_label)\n",
    "    overall_iou = intersection / (union + 1e-12)\n",
    "    \n",
    "    return overall_iou,iou_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "e995be69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the model activations and iou for each dataset\n",
    "def get_activations_and_iou(data_loader):\n",
    "    activations_list = []\n",
    "    overall_iou_list = []\n",
    "    iou_by_class_list =[]\n",
    "    predictions_list = []\n",
    "\n",
    "    for image,label in tqdm(data_loader):\n",
    "        # get prediction and bottleneck activations\n",
    "        output,bottleneck_activations = model_(image)\n",
    "        # transform the predictions to the corresponding classes\n",
    "        output_class=torch.argmax(output, dim=1)\n",
    "        # from 4d [1,1024,16,16] to 2d: [256,1024]\n",
    "        instance_activation_reshaped = bottleneck_activations.clone().detach().numpy().squeeze(0).reshape(1024,256).transpose()\n",
    "        activations_list.append(instance_activation_reshaped)\n",
    "        \n",
    "        overall_iou,iou_list = IoU_per_class(output_class,label)\n",
    "        overall_iou_list.append(overall_iou)\n",
    "        iou_by_class_list.append(iou_list)\n",
    "        output_class = output_class.detach().numpy()[0]\n",
    "        predictions_list.append(output_class)\n",
    "    \n",
    "    return np.array(activations_list),np.array(overall_iou_list),np.array(iou_by_class_list),np.array(predictions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "02081c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:20<00:00,  2.07s/it]\n"
     ]
    }
   ],
   "source": [
    "cityscapes_activations, cityscapes_overall_iou, cityscapes_iou_by_class, cityscapes_predictions = get_activations_and_iou(cityscapes_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "dd13b42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:12<00:00,  1.23s/it]\n"
     ]
    }
   ],
   "source": [
    "synthia_activations, synthia_overall_iou, synthia_iou_by_class, synthia_predictions = get_activations_and_iou(synthia_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "75402ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_activations = np.concatenate((cityscapes_activations,synthia_activations),axis=0)\n",
    "combined_overall_iou = np.concatenate((cityscapes_overall_iou,synthia_overall_iou),axis=0)\n",
    "combined_iou_by_class = np.concatenate((cityscapes_iou_by_class,synthia_iou_by_class),axis=0)\n",
    "combined_predictions = np.concatenate((cityscapes_predictions,synthia_predictions),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "a0a1a476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 256, 256)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ad3b92",
   "metadata": {},
   "source": [
    "Save predictions to paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "439117e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cityscapes_prediction_folder = \"D:\\zsh\\graduation\\Graduation-project-domain-shift-image-2-image\\dataset\\cityscapes_predictions\"\n",
    "cityscapes_prediction_folder_relative = os.path.relpath(cityscapes_prediction_folder, start)\n",
    "\n",
    "synthia_prediction_folder = \"D:\\zsh\\graduation\\Graduation-project-domain-shift-image-2-image\\dataset\\SYNTHIA_256\\predictions\"\n",
    "synthia_prediction_folder_relative = os.path.relpath(synthia_prediction_folder, start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "9b7b0928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save also the relative paths of predictions, so that we could access it from the front-end\n",
    "prediction_paths = []\n",
    "for i, prediction in enumerate(combined_predictions):\n",
    "    name = df.iloc[i][\"name\"]\n",
    "    dataset = df.iloc[i][\"dataset\"]\n",
    "    prediction_img = class_to_color(prediction)\n",
    "    if dataset==\"Cityscapes\":\n",
    "        full_folder = cityscapes_prediction_folder\n",
    "        relative_folder = cityscapes_prediction_folder_relative\n",
    "    elif dataset==\"Synthia\":\n",
    "        full_folder = synthia_prediction_folder\n",
    "        relative_folder = synthia_prediction_folder_relative\n",
    "    \n",
    "    prediction_img = Image.fromarray(prediction_img.astype(np.uint8))\n",
    "    prediction_img.save(full_folder+\"\\\\\"+name)\n",
    "    prediction_paths.append(relative_folder+\"\\\\\"+name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "34e406ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 256, 1024)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_activations.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12183c2",
   "metadata": {},
   "source": [
    "# Create embeddings of model activations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781fcf8f",
   "metadata": {},
   "source": [
    "TODO: first need to rescale the activations, then calculate the t-SNE, PCA and UMAP embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "7d024ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_activations_2d = combined_activations.reshape(combined_activations.shape[0]*combined_activations.shape[1],-1) # 256 is 16x16, the activation \"image\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "99d8dd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the first image of the subset (which corresponds to image 14 in the dataset)\n",
    "# selected_index = 0\n",
    "# sample_activation_image = cityscapes_activations[selected_index]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(combined_activations_2d)\n",
    "activations_2d_scaled = scaler.transform(combined_activations_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "b84a6ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0027131  0.00476936 0.00203868 ... 0.00214339 0.83588994 0.00502305]\n",
      " [0.0027131  0.00476936 0.00203868 ... 0.00214339 0.6648636  0.00502305]\n",
      " [0.0027131  0.00476936 0.00203868 ... 0.00214339 0.7757536  0.00502305]\n",
      " ...\n",
      " [0.00660872 0.00101541 0.00484488 ... 0.00713281 1.         0.01297495]\n",
      " [0.00660872 0.00101541 0.00484488 ... 0.41884694 0.7247918  0.01297495]\n",
      " [0.00660872 0.00101541 0.00484488 ... 0.00713281 0.77313083 0.01297495]]\n"
     ]
    }
   ],
   "source": [
    "print(activations_2d_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "a4a5a8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the labels\n",
    "# label = cityscapes_labels_sample[selected_index]\n",
    "\n",
    "# load the class names\n",
    "class_names = [\"others\",\"road\",\"sidewalk\",\"vegetation\",\"sky\",\"car\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "c430ffd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations_tsne_embedding = TSNE(n_components=2).fit_transform(activations_2d_scaled)\n",
    "activations_pca_embedding = PCA(n_components=2).fit_transform(activations_2d_scaled)\n",
    "\n",
    "reducer = umap.UMAP()\n",
    "activations_umap_embedding = reducer.fit_transform(activations_2d_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f509f6e6",
   "metadata": {},
   "source": [
    "TODO: and then reshape it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "01539b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations_tsne_embedding_reshaped = np.reshape(activations_tsne_embedding,(2*sample_number,256,2))\n",
    "activations_pca_embedding_reshaped = np.reshape(activations_pca_embedding,(2*sample_number,256,2))\n",
    "activations_umap_embedding_reshaped = np.reshape(activations_umap_embedding,(2*sample_number,256,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "b0d97e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_activation_1 = activations_tsne_embedding_reshaped[0]\n",
    "example_activation_2 = activations_tsne_embedding_reshaped[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "e9cf30c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1ca8fc962e8>"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print out the results to see if it makes sense now\n",
    "plt.scatter(example_activation_1[:,0],example_activation_1[:,1])\n",
    "plt.scatter(example_activation_2[:,0],example_activation_2[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "a18c3831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the activations to the full df\n",
    "df_full[\"bottleneck_tsne_embedding\"] = activations_tsne_embedding_reshaped.tolist()\n",
    "df_full[\"bottleneck_pca_embedding\"] = activations_pca_embedding_reshaped.tolist()\n",
    "df_full[\"bottleneck_umap_embedding\"] = activations_umap_embedding_reshaped.tolist()\n",
    "\n",
    "# overall_iou\n",
    "df_full[\"overall_iou\"] = combined_overall_iou\n",
    "\n",
    "# iou by class\n",
    "iou_class_df = pd.DataFrame(combined_iou_by_class,columns = [\"other_iou\",\"road_iou\",\"sidewalk_iou\",\"vegetation_iou\",\"sky_iou\",\"car_iou\"])\n",
    "df_full = pd.concat((df_full,iou_class_df),axis=1)\n",
    "\n",
    "# paths of predictions\n",
    "df_full[\"prediction_path\"] = prediction_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847d7782",
   "metadata": {},
   "source": [
    "Why is it that the points are not really separating?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "21de5beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rearrange the columns in a fixed order\n",
    "df_rearrange = df_full[['name','dataset', 'image_path', 'label_path','prediction_path', 'similar_image_paths','similar_IoU_score', \n",
    "                       'other_ratio', 'road_ratio', 'sidewalk_ratio','vegetation_ratio', 'sky_ratio', 'car_ratio', \n",
    "                       'simple_tsne_1','simple_tsne_2', 'meaningful_tsne_1', 'meaningful_tsne_2',\"pca_1\",\"pca_2\",\n",
    "       'bottleneck_tsne_embedding','bottleneck_pca_embedding','bottleneck_umap_embedding', 'overall_iou', 'other_iou',\n",
    "       'road_iou', 'sidewalk_iou', 'vegetation_iou', 'sky_iou', 'car_iou']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "0ec2f920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate columns\n",
    "# df_rearrange = df_rearrange.loc[:,~df_rearrange.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "24d18aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['name', 'dataset', 'image_path', 'label_path', 'prediction_path',\n",
       "       'similar_image_paths', 'similar_IoU_score', 'other_ratio', 'road_ratio',\n",
       "       'sidewalk_ratio', 'vegetation_ratio', 'sky_ratio', 'car_ratio',\n",
       "       'simple_tsne_1', 'simple_tsne_2', 'meaningful_tsne_1',\n",
       "       'meaningful_tsne_2', 'pca_1', 'pca_2', 'bottleneck_tsne_embedding',\n",
       "       'bottleneck_pca_embedding', 'bottleneck_umap_embedding', 'overall_iou',\n",
       "       'other_iou', 'road_iou', 'sidewalk_iou', 'vegetation_iou', 'sky_iou',\n",
       "       'car_iou'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rearrange.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "827495de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rearrange.to_csv(\"system_df_small_sample.csv\",index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22212c7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
